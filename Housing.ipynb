{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import tarfile \n",
    "import urllib\n",
    "\n",
    "DOWNLOAD_ROOT = \"https://raw.githubusercontent.com/ageron/handson-ml2/master/\" \n",
    "HOUSING_PATH = os.path.join(\"datasets\", \"housing\") \n",
    "HOUSING_URL = DOWNLOAD_ROOT + \"datasets/housing/housing.tgz\" \n",
    "\n",
    "def fetch_housing_data( housing_url = HOUSING_URL, housing_path = HOUSING_PATH): \n",
    "\tos.makedirs( housing_path, exist_ok = True) \n",
    "\ttgz_path = os.path.join( housing_path, \"housing.tgz\") \n",
    "\turllib.request.urlretrieve( housing_url, tgz_path) \n",
    "\thousing_tgz = tarfile.open( tgz_path) \n",
    "\thousing_tgz.extractall( path = housing_path) \n",
    "\thousing_tgz.close()\n",
    "    \n",
    "\n",
    "fetch_housing_data()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>housing_median_age</th>\n",
       "      <th>total_rooms</th>\n",
       "      <th>total_bedrooms</th>\n",
       "      <th>population</th>\n",
       "      <th>households</th>\n",
       "      <th>median_income</th>\n",
       "      <th>median_house_value</th>\n",
       "      <th>ocean_proximity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-122.23</td>\n",
       "      <td>37.88</td>\n",
       "      <td>41.0</td>\n",
       "      <td>880.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>322.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>8.3252</td>\n",
       "      <td>452600.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-122.22</td>\n",
       "      <td>37.86</td>\n",
       "      <td>21.0</td>\n",
       "      <td>7099.0</td>\n",
       "      <td>1106.0</td>\n",
       "      <td>2401.0</td>\n",
       "      <td>1138.0</td>\n",
       "      <td>8.3014</td>\n",
       "      <td>358500.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-122.24</td>\n",
       "      <td>37.85</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1467.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>496.0</td>\n",
       "      <td>177.0</td>\n",
       "      <td>7.2574</td>\n",
       "      <td>352100.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-122.25</td>\n",
       "      <td>37.85</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1274.0</td>\n",
       "      <td>235.0</td>\n",
       "      <td>558.0</td>\n",
       "      <td>219.0</td>\n",
       "      <td>5.6431</td>\n",
       "      <td>341300.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-122.25</td>\n",
       "      <td>37.85</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1627.0</td>\n",
       "      <td>280.0</td>\n",
       "      <td>565.0</td>\n",
       "      <td>259.0</td>\n",
       "      <td>3.8462</td>\n",
       "      <td>342200.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20635</th>\n",
       "      <td>-121.09</td>\n",
       "      <td>39.48</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1665.0</td>\n",
       "      <td>374.0</td>\n",
       "      <td>845.0</td>\n",
       "      <td>330.0</td>\n",
       "      <td>1.5603</td>\n",
       "      <td>78100.0</td>\n",
       "      <td>INLAND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20636</th>\n",
       "      <td>-121.21</td>\n",
       "      <td>39.49</td>\n",
       "      <td>18.0</td>\n",
       "      <td>697.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>356.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>2.5568</td>\n",
       "      <td>77100.0</td>\n",
       "      <td>INLAND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20637</th>\n",
       "      <td>-121.22</td>\n",
       "      <td>39.43</td>\n",
       "      <td>17.0</td>\n",
       "      <td>2254.0</td>\n",
       "      <td>485.0</td>\n",
       "      <td>1007.0</td>\n",
       "      <td>433.0</td>\n",
       "      <td>1.7000</td>\n",
       "      <td>92300.0</td>\n",
       "      <td>INLAND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20638</th>\n",
       "      <td>-121.32</td>\n",
       "      <td>39.43</td>\n",
       "      <td>18.0</td>\n",
       "      <td>1860.0</td>\n",
       "      <td>409.0</td>\n",
       "      <td>741.0</td>\n",
       "      <td>349.0</td>\n",
       "      <td>1.8672</td>\n",
       "      <td>84700.0</td>\n",
       "      <td>INLAND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20639</th>\n",
       "      <td>-121.24</td>\n",
       "      <td>39.37</td>\n",
       "      <td>16.0</td>\n",
       "      <td>2785.0</td>\n",
       "      <td>616.0</td>\n",
       "      <td>1387.0</td>\n",
       "      <td>530.0</td>\n",
       "      <td>2.3886</td>\n",
       "      <td>89400.0</td>\n",
       "      <td>INLAND</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20640 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\n",
       "0        -122.23     37.88                41.0        880.0           129.0   \n",
       "1        -122.22     37.86                21.0       7099.0          1106.0   \n",
       "2        -122.24     37.85                52.0       1467.0           190.0   \n",
       "3        -122.25     37.85                52.0       1274.0           235.0   \n",
       "4        -122.25     37.85                52.0       1627.0           280.0   \n",
       "...          ...       ...                 ...          ...             ...   \n",
       "20635    -121.09     39.48                25.0       1665.0           374.0   \n",
       "20636    -121.21     39.49                18.0        697.0           150.0   \n",
       "20637    -121.22     39.43                17.0       2254.0           485.0   \n",
       "20638    -121.32     39.43                18.0       1860.0           409.0   \n",
       "20639    -121.24     39.37                16.0       2785.0           616.0   \n",
       "\n",
       "       population  households  median_income  median_house_value  \\\n",
       "0           322.0       126.0         8.3252            452600.0   \n",
       "1          2401.0      1138.0         8.3014            358500.0   \n",
       "2           496.0       177.0         7.2574            352100.0   \n",
       "3           558.0       219.0         5.6431            341300.0   \n",
       "4           565.0       259.0         3.8462            342200.0   \n",
       "...           ...         ...            ...                 ...   \n",
       "20635       845.0       330.0         1.5603             78100.0   \n",
       "20636       356.0       114.0         2.5568             77100.0   \n",
       "20637      1007.0       433.0         1.7000             92300.0   \n",
       "20638       741.0       349.0         1.8672             84700.0   \n",
       "20639      1387.0       530.0         2.3886             89400.0   \n",
       "\n",
       "      ocean_proximity  \n",
       "0            NEAR BAY  \n",
       "1            NEAR BAY  \n",
       "2            NEAR BAY  \n",
       "3            NEAR BAY  \n",
       "4            NEAR BAY  \n",
       "...               ...  \n",
       "20635          INLAND  \n",
       "20636          INLAND  \n",
       "20637          INLAND  \n",
       "20638          INLAND  \n",
       "20639          INLAND  \n",
       "\n",
       "[20640 rows x 10 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_housing_data(housing_path=HOUSING_PATH):\n",
    "    csv_path = os.path.join(housing_path, \"housing.csv\")\n",
    "    return pd.read_csv(csv_path)\n",
    "\n",
    "load_housing_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>housing_median_age</th>\n",
       "      <th>total_rooms</th>\n",
       "      <th>total_bedrooms</th>\n",
       "      <th>population</th>\n",
       "      <th>households</th>\n",
       "      <th>median_income</th>\n",
       "      <th>median_house_value</th>\n",
       "      <th>ocean_proximity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-122.23</td>\n",
       "      <td>37.88</td>\n",
       "      <td>41.0</td>\n",
       "      <td>880.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>322.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>8.3252</td>\n",
       "      <td>452600.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-122.22</td>\n",
       "      <td>37.86</td>\n",
       "      <td>21.0</td>\n",
       "      <td>7099.0</td>\n",
       "      <td>1106.0</td>\n",
       "      <td>2401.0</td>\n",
       "      <td>1138.0</td>\n",
       "      <td>8.3014</td>\n",
       "      <td>358500.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-122.24</td>\n",
       "      <td>37.85</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1467.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>496.0</td>\n",
       "      <td>177.0</td>\n",
       "      <td>7.2574</td>\n",
       "      <td>352100.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-122.25</td>\n",
       "      <td>37.85</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1274.0</td>\n",
       "      <td>235.0</td>\n",
       "      <td>558.0</td>\n",
       "      <td>219.0</td>\n",
       "      <td>5.6431</td>\n",
       "      <td>341300.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-122.25</td>\n",
       "      <td>37.85</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1627.0</td>\n",
       "      <td>280.0</td>\n",
       "      <td>565.0</td>\n",
       "      <td>259.0</td>\n",
       "      <td>3.8462</td>\n",
       "      <td>342200.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\n",
       "0    -122.23     37.88                41.0        880.0           129.0   \n",
       "1    -122.22     37.86                21.0       7099.0          1106.0   \n",
       "2    -122.24     37.85                52.0       1467.0           190.0   \n",
       "3    -122.25     37.85                52.0       1274.0           235.0   \n",
       "4    -122.25     37.85                52.0       1627.0           280.0   \n",
       "\n",
       "   population  households  median_income  median_house_value ocean_proximity  \n",
       "0       322.0       126.0         8.3252            452600.0        NEAR BAY  \n",
       "1      2401.0      1138.0         8.3014            358500.0        NEAR BAY  \n",
       "2       496.0       177.0         7.2574            352100.0        NEAR BAY  \n",
       "3       558.0       219.0         5.6431            341300.0        NEAR BAY  \n",
       "4       565.0       259.0         3.8462            342200.0        NEAR BAY  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_housing_data(housing_path=HOUSING_PATH):\n",
    "    csv_path = os.path.join(housing_path, \"housing.csv\")\n",
    "    return pd.read_csv(csv_path)\n",
    "\n",
    "housing = load_housing_data()\n",
    "\n",
    "housing.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20640 entries, 0 to 20639\n",
      "Data columns (total 10 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   longitude           20640 non-null  float64\n",
      " 1   latitude            20640 non-null  float64\n",
      " 2   housing_median_age  20640 non-null  float64\n",
      " 3   total_rooms         20640 non-null  float64\n",
      " 4   total_bedrooms      20433 non-null  float64\n",
      " 5   population          20640 non-null  float64\n",
      " 6   households          20640 non-null  float64\n",
      " 7   median_income       20640 non-null  float64\n",
      " 8   median_house_value  20640 non-null  float64\n",
      " 9   ocean_proximity     20640 non-null  object \n",
      "dtypes: float64(9), object(1)\n",
      "memory usage: 1.6+ MB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_housing_data(housing_path=HOUSING_PATH):\n",
    "    csv_path = os.path.join(housing_path, \"housing.csv\")\n",
    "    return pd.read_csv(csv_path)\n",
    "\n",
    "housing = load_housing_data()\n",
    "\n",
    "housing.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1H OCEAN     9136\n",
       "INLAND        6551\n",
       "NEAR OCEAN    2658\n",
       "NEAR BAY      2290\n",
       "ISLAND           5\n",
       "Name: ocean_proximity, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_housing_data(housing_path=HOUSING_PATH):\n",
    "    csv_path = os.path.join(housing_path, \"housing.csv\")\n",
    "    return pd.read_csv(csv_path)\n",
    "\n",
    "housing = load_housing_data()\n",
    "\n",
    "housing[\"ocean_proximity\"]. value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>housing_median_age</th>\n",
       "      <th>total_rooms</th>\n",
       "      <th>total_bedrooms</th>\n",
       "      <th>population</th>\n",
       "      <th>households</th>\n",
       "      <th>median_income</th>\n",
       "      <th>median_house_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>20640.000000</td>\n",
       "      <td>20640.000000</td>\n",
       "      <td>20640.000000</td>\n",
       "      <td>20640.000000</td>\n",
       "      <td>20433.000000</td>\n",
       "      <td>20640.000000</td>\n",
       "      <td>20640.000000</td>\n",
       "      <td>20640.000000</td>\n",
       "      <td>20640.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-119.569704</td>\n",
       "      <td>35.631861</td>\n",
       "      <td>28.639486</td>\n",
       "      <td>2635.763081</td>\n",
       "      <td>537.870553</td>\n",
       "      <td>1425.476744</td>\n",
       "      <td>499.539680</td>\n",
       "      <td>3.870671</td>\n",
       "      <td>206855.816909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.003532</td>\n",
       "      <td>2.135952</td>\n",
       "      <td>12.585558</td>\n",
       "      <td>2181.615252</td>\n",
       "      <td>421.385070</td>\n",
       "      <td>1132.462122</td>\n",
       "      <td>382.329753</td>\n",
       "      <td>1.899822</td>\n",
       "      <td>115395.615874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-124.350000</td>\n",
       "      <td>32.540000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.499900</td>\n",
       "      <td>14999.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-121.800000</td>\n",
       "      <td>33.930000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>1447.750000</td>\n",
       "      <td>296.000000</td>\n",
       "      <td>787.000000</td>\n",
       "      <td>280.000000</td>\n",
       "      <td>2.563400</td>\n",
       "      <td>119600.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-118.490000</td>\n",
       "      <td>34.260000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>2127.000000</td>\n",
       "      <td>435.000000</td>\n",
       "      <td>1166.000000</td>\n",
       "      <td>409.000000</td>\n",
       "      <td>3.534800</td>\n",
       "      <td>179700.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>-118.010000</td>\n",
       "      <td>37.710000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>3148.000000</td>\n",
       "      <td>647.000000</td>\n",
       "      <td>1725.000000</td>\n",
       "      <td>605.000000</td>\n",
       "      <td>4.743250</td>\n",
       "      <td>264725.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>-114.310000</td>\n",
       "      <td>41.950000</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>39320.000000</td>\n",
       "      <td>6445.000000</td>\n",
       "      <td>35682.000000</td>\n",
       "      <td>6082.000000</td>\n",
       "      <td>15.000100</td>\n",
       "      <td>500001.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          longitude      latitude  housing_median_age   total_rooms  \\\n",
       "count  20640.000000  20640.000000        20640.000000  20640.000000   \n",
       "mean    -119.569704     35.631861           28.639486   2635.763081   \n",
       "std        2.003532      2.135952           12.585558   2181.615252   \n",
       "min     -124.350000     32.540000            1.000000      2.000000   \n",
       "25%     -121.800000     33.930000           18.000000   1447.750000   \n",
       "50%     -118.490000     34.260000           29.000000   2127.000000   \n",
       "75%     -118.010000     37.710000           37.000000   3148.000000   \n",
       "max     -114.310000     41.950000           52.000000  39320.000000   \n",
       "\n",
       "       total_bedrooms    population    households  median_income  \\\n",
       "count    20433.000000  20640.000000  20640.000000   20640.000000   \n",
       "mean       537.870553   1425.476744    499.539680       3.870671   \n",
       "std        421.385070   1132.462122    382.329753       1.899822   \n",
       "min          1.000000      3.000000      1.000000       0.499900   \n",
       "25%        296.000000    787.000000    280.000000       2.563400   \n",
       "50%        435.000000   1166.000000    409.000000       3.534800   \n",
       "75%        647.000000   1725.000000    605.000000       4.743250   \n",
       "max       6445.000000  35682.000000   6082.000000      15.000100   \n",
       "\n",
       "       median_house_value  \n",
       "count        20640.000000  \n",
       "mean        206855.816909  \n",
       "std         115395.615874  \n",
       "min          14999.000000  \n",
       "25%         119600.000000  \n",
       "50%         179700.000000  \n",
       "75%         264725.000000  \n",
       "max         500001.000000  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_housing_data(housing_path=HOUSING_PATH):\n",
    "    csv_path = os.path.join(housing_path, \"housing.csv\")\n",
    "    return pd.read_csv(csv_path)\n",
    "\n",
    "housing = load_housing_data()\n",
    "\n",
    "housing.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'save_fig' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-0bc29ae8247b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mhousing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbins\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m11\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0msave_fig\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"attribute_histogram_plots\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'save_fig' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 792x576 with 9 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_housing_data(housing_path=HOUSING_PATH):\n",
    "    csv_path = os.path.join(housing_path, \"housing.csv\")\n",
    "    return pd.read_csv(csv_path)\n",
    "\n",
    "housing = load_housing_data()\n",
    "\n",
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt\n",
    "housing.hist(bins=50, figsize=(11,8))\n",
    "save_fig(\"attribute_histogram_plots\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_housing_data(housing_path=HOUSING_PATH):\n",
    "    csv_path = os.path.join(housing_path, \"housing.csv\")\n",
    "    return pd.read_csv(csv_path)\n",
    "\n",
    "housing = load_housing_data()\n",
    "\n",
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt \n",
    "housing.hist( bins = 50, figsize =( 20,15)) \n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random as rnd\n",
    "\n",
    "# to make this notebook's output identical at every run\n",
    "# this makes sure the training set and test set are always the same\n",
    "# which in turn makes sure of something more important...\n",
    "# that the learning algorithm doesn't see the test set! \n",
    "rnd.seed(42) \n",
    "\n",
    "def split_train_test(data, test_ratio):\n",
    "    #creates randomly ordered unique indices spanning length of data\n",
    "    shuffled_indices = rnd.permutation(len(data))\n",
    "    #in our case 20% * the total elements\n",
    "    test_set_size = int(len(data) * test_ratio)\n",
    "    #get the train and test sets using random indices and proper sizes\n",
    "    test_indices = shuffled_indices[:test_set_size]\n",
    "    train_indices = shuffled_indices[test_set_size:]\n",
    "    return data.iloc[train_indices], data.iloc[test_indices]\n",
    "\n",
    "train_set, test_set = split_train_test(housing, 0.2)\n",
    "print(len(train_set), len(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zlib import crc32 \n",
    "import numpy as np\n",
    "import numpy.random as rnd\n",
    "\n",
    "#this function returns a boolean\n",
    "#using crc32(np.int64(identifier)) & 0xffffffff\n",
    "#this ensures the same identifier returns the same hash so\n",
    "#that way the test set is always the same since the hash is always the same\n",
    "#as is the boolean returned which is what organizes the set\n",
    "#new incoming data has an 80% chance of hashing itself to training\n",
    "#and 20% of hashing into test\n",
    "#this makes sure the test data is always the same\n",
    "#same with the training data, and that new data will be ensured to land\n",
    "#with correct probability into one of these areas\n",
    "def test_set_check(identifier, test_ratio): \n",
    "    return crc32(np.int64(identifier)) & 0xffffffff < test_ratio * 2** 32 \n",
    "\n",
    "#our test_ratio is 0.2\n",
    "#the id is just the row since data has no unique identifier\n",
    "def split_train_test_by_id(data, test_ratio, id_column): \n",
    "    ids = data[id_column] \n",
    "    #for each id apply the hash that is bitwise and operated on\n",
    "    in_test_set = ids.apply(lambda id_:test_set_check(id_,test_ratio)) \n",
    "    return data.loc[~in_test_set], data.loc[ in_test_set]\n",
    "\n",
    "def split_train_test(data, test_ratio):\n",
    "    #creates randomly ordered unique indices spanning length of data\n",
    "    shuffled_indices = rnd.permutation(len(data))\n",
    "    #in our case 20% * the total elements\n",
    "    test_set_size = int(len(data) * test_ratio)\n",
    "    #get the train and test sets using random indices and proper sizes\n",
    "    test_indices = shuffled_indices[:test_set_size]\n",
    "    train_indices = shuffled_indices[test_set_size:]\n",
    "    return data.iloc[train_indices], data.iloc[test_indices]\n",
    "\n",
    "\n",
    "def load_housing_data(housing_path=HOUSING_PATH):\n",
    "    csv_path = os.path.join(housing_path, \"housing.csv\")\n",
    "    return pd.read_csv(csv_path)\n",
    "\n",
    "housing = load_housing_data()\n",
    "\n",
    "# adds an ` index ` column\n",
    "housing_with_id = housing.reset_index()\n",
    "\n",
    "train_set, test_set = split_train_test_by_id(housing_with_id, 0.2, \"index\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zlib import crc32 \n",
    "import numpy as np\n",
    "import numpy.random as rnd\n",
    "\n",
    "#this function returns a boolean\n",
    "#using crc32(np.int64(identifier)) & 0xffffffff\n",
    "#this ensures the same identifier returns the same hash so\n",
    "#that way the test set is always the same since the hash is always the same\n",
    "#as is the boolean returned which is what organizes the set\n",
    "#new incoming data has an 80% chance of hashing itself to training\n",
    "#and 20% of hashing into test\n",
    "#this makes sure the test data is always the same\n",
    "#same with the training data, and that new data will be ensured to land\n",
    "#with correct probability into one of these areas\n",
    "def test_set_check(identifier, test_ratio): \n",
    "    return crc32(np.int64(identifier)) & 0xffffffff < test_ratio * 2** 32 \n",
    "\n",
    "#our test_ratio is 0.2\n",
    "#the id is just the row since data has no unique identifier\n",
    "def split_train_test_by_id(data, test_ratio, id_column): \n",
    "    ids = data[id_column] \n",
    "    #for each id apply the hash that is bitwise and operated on\n",
    "    in_test_set = ids.apply(lambda id_:test_set_check(id_,test_ratio)) \n",
    "    return data.loc[~in_test_set], data.loc[ in_test_set]\n",
    "\n",
    "def split_train_test(data, test_ratio):\n",
    "    #creates randomly ordered unique indices spanning length of data\n",
    "    shuffled_indices = rnd.permutation(len(data))\n",
    "    #in our case 20% * the total elements\n",
    "    test_set_size = int(len(data) * test_ratio)\n",
    "    #get the train and test sets using random indices and proper sizes\n",
    "    test_indices = shuffled_indices[:test_set_size]\n",
    "    train_indices = shuffled_indices[test_set_size:]\n",
    "    return data.iloc[train_indices], data.iloc[test_indices]\n",
    "\n",
    "\n",
    "def load_housing_data(housing_path=HOUSING_PATH):\n",
    "    csv_path = os.path.join(housing_path, \"housing.csv\")\n",
    "    return pd.read_csv(csv_path)\n",
    "\n",
    "housing = load_housing_data()\n",
    "\n",
    "# adds an ` index ` column\n",
    "housing_with_id = housing.reset_index()\n",
    "\n",
    "train_set, test_set = split_train_test_by_id(housing_with_id, 0.2, \"index\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zlib import crc32 \n",
    "import numpy as np\n",
    "import numpy.random as rnd\n",
    "\n",
    "#this function returns a boolean\n",
    "#using crc32(np.int64(identifier)) & 0xffffffff\n",
    "#this ensures the same identifier returns the same hash so\n",
    "#that way the test set is always the same since the hash is always the same\n",
    "#as is the boolean returned which is what organizes the set\n",
    "#new incoming data has an 80% chance of hashing itself to training\n",
    "#and 20% of hashing into test\n",
    "#this makes sure the test data is always the same\n",
    "#same with the training data, and that new data will be ensured to land\n",
    "#with correct probability into one of these areas\n",
    "def test_set_check(identifier, test_ratio): \n",
    "    return crc32(np.int64(identifier)) & 0xffffffff < test_ratio * 2** 32 \n",
    "\n",
    "#our test_ratio is 0.2\n",
    "#the id is just the row since data has no unique identifier\n",
    "def split_train_test_by_id(data, test_ratio, id_column): \n",
    "    ids = data[id_column] \n",
    "    #for each id apply the hash that is bitwise and operated on\n",
    "    in_test_set = ids.apply(lambda id_:test_set_check(id_,test_ratio)) \n",
    "    return data.loc[~in_test_set], data.loc[ in_test_set]\n",
    "\n",
    "def split_train_test(data, test_ratio):\n",
    "    #creates randomly ordered unique indices spanning length of data\n",
    "    shuffled_indices = rnd.permutation(len(data))\n",
    "    #in our case 20% * the total elements\n",
    "    test_set_size = int(len(data) * test_ratio)\n",
    "    #get the train and test sets using random indices and proper sizes\n",
    "    test_indices = shuffled_indices[:test_set_size]\n",
    "    train_indices = shuffled_indices[test_set_size:]\n",
    "    return data.iloc[train_indices], data.iloc[test_indices]\n",
    "\n",
    "\n",
    "def load_housing_data(housing_path=HOUSING_PATH):\n",
    "    csv_path = os.path.join(housing_path, \"housing.csv\")\n",
    "    return pd.read_csv(csv_path)\n",
    "\n",
    "housing = load_housing_data()\n",
    "\n",
    "\n",
    "\n",
    "train_set, test_set = split_train_test_by_id(housing_with_id, 0.2, \"index\")\n",
    "\n",
    "print(len(train_set), len(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_housing_data(housing_path=HOUSING_PATH):\n",
    "    csv_path = os.path.join(housing_path, \"housing.csv\")\n",
    "    return pd.read_csv(csv_path)\n",
    "\n",
    "load_housing_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zlib import crc32 \n",
    "import numpy as np\n",
    "import numpy.random as rnd\n",
    "\n",
    "#this function returns a boolean\n",
    "#using crc32(np.int64(identifier)) & 0xffffffff\n",
    "#this ensures the same identifier returns the same hash so\n",
    "#that way the test set is always the same since the hash is always the same\n",
    "#as is the boolean returned which is what organizes the set\n",
    "#new incoming data has an 80% chance of hashing itself to training\n",
    "#and 20% of hashing into test\n",
    "#this makes sure the test data is always the same\n",
    "#same with the training data, and that new data will be ensured to land\n",
    "#with correct probability into one of these areas\n",
    "def test_set_check(identifier, test_ratio): \n",
    "    return crc32(np.int64(identifier)) & 0xffffffff < test_ratio * 2** 32 \n",
    "\n",
    "#our test_ratio is 0.2\n",
    "#the id is just the row since data has no unique identifier\n",
    "def split_train_test_by_id(data, test_ratio, id_column): \n",
    "    ids = data[id_column] \n",
    "    #for each id apply the hash that is bitwise and operated on\n",
    "    in_test_set = ids.apply(lambda id_:test_set_check(id_,test_ratio)) \n",
    "    return data.loc[~in_test_set], data.loc[ in_test_set]\n",
    "\n",
    "def split_train_test(data, test_ratio):\n",
    "    #creates randomly ordered unique indices spanning length of data\n",
    "    shuffled_indices = rnd.permutation(len(data))\n",
    "    #in our case 20% * the total elements\n",
    "    test_set_size = int(len(data) * test_ratio)\n",
    "    #get the train and test sets using random indices and proper sizes\n",
    "    test_indices = shuffled_indices[:test_set_size]\n",
    "    train_indices = shuffled_indices[test_set_size:]\n",
    "    return data.iloc[train_indices], data.iloc[test_indices]\n",
    "\n",
    "\n",
    "def load_housing_data(housing_path=HOUSING_PATH):\n",
    "    csv_path = os.path.join(housing_path, \"housing.csv\")\n",
    "    return pd.read_csv(csv_path)\n",
    "\n",
    "housing = load_housing_data()\n",
    "\n",
    "# adds an ` index ` column\n",
    "housing_with_id = housing.reset_index()\n",
    "\n",
    "housing_with_id.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zlib import crc32 \n",
    "import numpy as np\n",
    "import numpy.random as rnd\n",
    "\n",
    "#this function returns a boolean\n",
    "#using crc32(np.int64(identifier)) & 0xffffffff\n",
    "#this ensures the same identifier returns the same hash so\n",
    "#that way the test set is always the same since the hash is always the same\n",
    "#as is the boolean returned which is what organizes the set\n",
    "#new incoming data has an 80% chance of hashing itself to training\n",
    "#and 20% of hashing into test\n",
    "#this makes sure the test data is always the same\n",
    "#same with the training data, and that new data will be ensured to land\n",
    "#with correct probability into one of these areas\n",
    "def test_set_check(identifier, test_ratio): \n",
    "    return crc32(np.int64(identifier)) & 0xffffffff < test_ratio * 2** 32 \n",
    "\n",
    "#our test_ratio is 0.2\n",
    "#the id is just the row since data has no unique identifier\n",
    "def split_train_test_by_id(data, test_ratio, id_column): \n",
    "    ids = data[id_column] \n",
    "    #for each id apply the hash that is bitwise and operated on\n",
    "    in_test_set = ids.apply(lambda id_:test_set_check(id_,test_ratio)) \n",
    "    return data.loc[~in_test_set], data.loc[ in_test_set]\n",
    "\n",
    "def split_train_test(data, test_ratio):\n",
    "    #creates randomly ordered unique indices spanning length of data\n",
    "    shuffled_indices = rnd.permutation(len(data))\n",
    "    #in our case 20% * the total elements\n",
    "    test_set_size = int(len(data) * test_ratio)\n",
    "    #get the train and test sets using random indices and proper sizes\n",
    "    test_indices = shuffled_indices[:test_set_size]\n",
    "    train_indices = shuffled_indices[test_set_size:]\n",
    "    return data.iloc[train_indices], data.iloc[test_indices]\n",
    "\n",
    "\n",
    "def load_housing_data(housing_path=HOUSING_PATH):\n",
    "    csv_path = os.path.join(housing_path, \"housing.csv\")\n",
    "    return pd.read_csv(csv_path)\n",
    "\n",
    "housing = load_housing_data()\n",
    "\n",
    "# adds an ` index ` column\n",
    "housing_with_id = housing.reset_index()\n",
    "\n",
    "housing_with_id.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "def load_housing_data(housing_path=HOUSING_PATH):\n",
    "    csv_path = os.path.join(housing_path, \"housing.csv\")\n",
    "    return pd.read_csv(csv_path)\n",
    "\n",
    "\n",
    "\n",
    "housing = load_housing_data()\n",
    "\n",
    "train_setSci, test_setSci = train_test_split( housing, test_size = 0.2, random_state = 42)\n",
    "\n",
    "print(len(train_setSci), len(test)setSci)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "def load_housing_data(housing_path=HOUSING_PATH):\n",
    "    csv_path = os.path.join(housing_path, \"housing.csv\")\n",
    "    return pd.read_csv(csv_path)\n",
    "\n",
    "\n",
    "\n",
    "housing = load_housing_data()\n",
    "\n",
    "train_setSci, test_setSci = train_test_split( housing, test_size = 0.2, random_state = 42)\n",
    "\n",
    "print(len(train_setSci), len(test)setSci)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "def load_housing_data(housing_path=HOUSING_PATH):\n",
    "    csv_path = os.path.join(housing_path, \"housing.csv\")\n",
    "    return pd.read_csv(csv_path)\n",
    "\n",
    "\n",
    "\n",
    "housing = load_housing_data()\n",
    "\n",
    "\n",
    "#so for future reference this is the way to go, way less code to do the same thing\n",
    "#stand on the shoulder of giants\n",
    "train_setSci, test_setSci = train_test_split( housing, test_size = 0.2, random_state = 42)\n",
    "\n",
    "print(len(train_setSci), len(test_setSci))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_housing_data(housing_path=HOUSING_PATH):\n",
    "    csv_path = os.path.join(housing_path, \"housing.csv\")\n",
    "    return pd.read_csv(csv_path)\n",
    "\n",
    "\n",
    "\n",
    "housing = load_housing_data()\n",
    "\n",
    "housing[\"income_cat\"] = pd.cut( housing[\"median_income\"], bins =[ 0., 1.5, 3.0, 4.5, 6., np.inf], labels =[ 1, 2, 3, 4, 5])\n",
    "\n",
    "housing[\"income_cat\"]. hist()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_housing_data(housing_path=HOUSING_PATH):\n",
    "    csv_path = os.path.join(housing_path, \"housing.csv\")\n",
    "    return pd.read_csv(csv_path)\n",
    "\n",
    "\n",
    "\n",
    "housing = load_housing_data()\n",
    "\n",
    "housing[\"income_cat\"] = pd.cut( housing[\"median_income\"], bins =[ 0., 1.5, 3.0, 4.5, 6., np.inf], labels =[ 1, 2, 3, 4, 5])\n",
    "\n",
    "housing[\"income_cat\"]. hist()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "\n",
    "\n",
    "#here we can see in the ouput the stratified sampling method samples properly \n",
    "#based on the histograms display that category 3 is more prominent than 2\n",
    "#than 4 than 5 than 1\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "for train_index, test_index in split.split(housing, housing[\"income_cat\"]):\n",
    "    strat_train_set = housing.loc[train_index]\n",
    "    strat_test_set = housing.loc[test_index]    \n",
    "    \n",
    "\n",
    "strat_test_set[\"income_cat\"].value_counts()/len(strat_test_set)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def income_cat_proportions(data):\n",
    "    return data[\"income_cat\"].value_counts() / len(data)\n",
    "\n",
    "train_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)\n",
    "\n",
    "compare_props = pd.DataFrame({\n",
    "    \"Overall\": income_cat_proportions(housing),\n",
    "    \"Stratified\": income_cat_proportions(strat_test_set),\n",
    "    \"Random\": income_cat_proportions(test_set),\n",
    "}).sort_index()\n",
    "compare_props[\"Rand. %error\"] = 100 * compare_props[\"Random\"] / compare_props[\"Overall\"] - 100\n",
    "compare_props[\"Strat. %error\"] = 100 * compare_props[\"Stratified\"] / compare_props[\"Overall\"] - 100\n",
    "\n",
    "compare_props"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = load_housing_data()\n",
    "\n",
    "# adds an ` index ` column\n",
    "housing_with_id = housing.reset_index()\n",
    "\n",
    "housing_with_id.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for set_ in (strat_train_set, strat_test_set): \n",
    "    set_. drop(\"income_cat\", axis = 1, inplace = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for set_ in (strat_train_set, strat_test_set): \n",
    "    set_.drop(\"income_cat\", axis = 1, inplace = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = strat_train_set.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.plot( kind =\"scatter\", x =\"longitude\", y =\"latitude\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.plot( kind =\"scatter\", x =\"longitude\", y =\"latitude\", alpha=0.1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\",\n",
    "    \n",
    "    #this controls the radius of the image in the circle\n",
    "    #larger radius larger population\n",
    "    s=housing['population']/100, label=\"population\",\n",
    "    #the color is based on house value\n",
    "    #blue is cheap red expensive\n",
    "    c=\"median_house_value\", cmap=plt.get_cmap(\"jet\"),\n",
    "    colorbar=True, alpha=0.4, figsize=(10,7),\n",
    ")\n",
    "plt.legend()\n",
    "save_fig(\"housing_prices_scatterplot\")\n",
    "plt.show()\n",
    "\n",
    "#as seen in the plot, houses closer to the ocean are typically more expensive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate the standard correlation between median_house_value and every other attribute\n",
    "corr_matrix[\"median_house_value\"]. sort_values( ascending = False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate the standard correlation between median_house_value and every other attribute\n",
    "corr_matrix = housing.corr()\n",
    "\n",
    "corr_matrix[\" median_house_value\"]. sort_values( ascending = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate the standard correlation between median_house_value and every other attribute\n",
    "corr_matrix = housing.corr()\n",
    "\n",
    "corr_matrix[\"median_house_value\"]. sort_values( ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "attributes = [\"median_house_value\", \"median_income\", \"total_rooms\", \"housing_median_age\"]\n",
    "scatter_matrix(housing[attributes], figsize=(11, 8))\n",
    "save_fig(\"scatter_matrix_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "attributes = [\"median_house_value\", \"median_income\", \"total_rooms\", \"housing_median_age\"]\n",
    "scatter_matrix(housing[attributes], figsize=(11, 8))\n",
    "save_fig(\"scatter_matrix_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.image as mpimg\n",
    "\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "california_img=mpimg.imread('datasets/housing/california.png')\n",
    "ax = housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", figsize=(10,7),\n",
    "                       s=housing['population']/100, label=\"Population\",\n",
    "                       c=\"median_house_value\", cmap=plt.get_cmap(\"jet\"),\n",
    "                       colorbar=False, alpha=0.4,\n",
    "                      )\n",
    "plt.imshow(california_img, extent=[-124.55, -113.80, 32.45, 42.05], alpha=0.5)\n",
    "plt.ylabel(\"Latitude\", fontsize=14)\n",
    "plt.xlabel(\"Longitude\", fontsize=14)\n",
    "\n",
    "prices = housing[\"median_house_value\"]\n",
    "tick_values = np.linspace(prices.min(), prices.max(), 11)\n",
    "cbar = plt.colorbar()\n",
    "cbar.ax.set_yticklabels([\"$%dk\"%(round(v/1000)) for v in tick_values], fontsize=14)\n",
    "cbar.set_label('Median House Value', fontsize=16)\n",
    "\n",
    "plt.legend(fontsize=16)\n",
    "save_fig(\"california_housing_prices_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "attributes = [\"median_house_value\", \"median_income\", \"total_rooms\", \"housing_median_age\"]\n",
    "scatter_matrix(housing[attributes], figsize=(11, 8))\n",
    "save_fig(\"scatter_matrix_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#median income is most promising to predict house value\n",
    "housing.plot( kind =\"scatter\", x =\"median_income\", y =\"median_house_value\", alpha = 0.1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create new attributes as a combination of existing ones\n",
    "housing[\"rooms_per_household\"] = housing[\"total_rooms\"] / housing[\"population\"]\n",
    "housing[\"bedrooms_per_room\"] = housing[\"total_bedrooms\"] / housing[\"total_rooms\"]\n",
    "housing[\"population_per_household\"] = housing[\"population\"] / housing[\"households\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check correlations of median_house_value now that we added some new combination attributes\n",
    "corr_matrix = housing.corr()\n",
    "corr_matrix[\"median_house_value\"].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#note there is a positive correlation between bedrooms and house value, and a positive correlation \n",
    "#between total rooms and house value, but there is a negative correlation between bedrooms_per_room\n",
    "#and house value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing[\"bedrooms_per_latitude\"] = housing[\"latitude\"] / housing[\"total_bedrooms\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.plot( kind =\"scatter\", x =\"latitude\", y =\"total_bedrooms\", alpha = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove price since its the target value\n",
    "#this creates a copy without the value to drop\n",
    "#strat_train_set is not affected\n",
    "housing = strat_train_set.drop(\"median_house_value\", axis=1)\n",
    "#copy the labels a.k.a. the targets\n",
    "housing_labels = strat_train_set[\"median_house_value\"].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for total bedrooms a couple hundred district did not have values...\n",
    "#let's set the bedroom value to the median value of the training set\n",
    "#in case for some reason later on we see a correlation here...\n",
    "#good thing we made a copy of the data before this step..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median = train_set[\"total_bedrooms\"].median()\n",
    "# option 3\n",
    "housing_copy[\"total_bedrooms\"].fillna(median, inplace=True) \n",
    "housing_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median = train_set[\"total_bedrooms\"].median()\n",
    "# option 3\n",
    "housing[\"total_bedrooms\"].fillna(median, inplace=True) \n",
    "housing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(16512+4128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_set[\"total_bedrooms\"].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing[70]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.loc(90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.loc[90]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.loc[90]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.loc[90] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.loc[90]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Impute\n",
    "\n",
    "#fill empty spots using median of dataset for that column\n",
    "imputer = Imputer(strategy='median')\n",
    "#need to drop any non numerical columns\n",
    "housing_num = housing.drop(\"ocean_proximity\", axis=1)\n",
    "imputer.fit(housing_num)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Impute\n",
    "\n",
    "#fill empty spots using median of dataset for that column\n",
    "imputer = Imputer(strategy='median')\n",
    "#need to drop any non numerical columns\n",
    "housing_num = housing.drop(\"ocean_proximity\", axis=1)\n",
    "imputer.fit(housing_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import SimpleImputer\n",
    "\n",
    "#fill empty spots using median of dataset for that column\n",
    "imputer = Imputer(strategy='median')\n",
    "#need to drop any non numerical columns\n",
    "housing_num = housing.drop(\"ocean_proximity\", axis=1)\n",
    "imputer.fit(housing_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "\n",
    "#fill empty spots using median of dataset for that column\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "#need to drop any non numerical columns\n",
    "housing_num = housing.drop(\"ocean_proximity\", axis=1)\n",
    "imputer.fit(housing_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "\n",
    "#fill empty spots using median of dataset for that column\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "#need to drop any non numerical columns\n",
    "housing_num = housing.drop(\"ocean_proximity\", axis=1)\n",
    "imputer.fit(housing_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.loc[90]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the calculated median per column\n",
    "imputer.statistics_\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the imputer matches the actual median vals\n",
    "housing_num.median().values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fill in the blank spots with median values calculated by imputer\n",
    "X = imputer.transform(housing_num)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.loc[90]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_num.loc[90]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#put the data back into the data frame\n",
    "housing_tr = pd.DataFrame( X, columns = housing_num.columns, index = housing_num.index)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_num.loc[90]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.loc[90]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.loc[91]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X[90])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_num.loc[90] = X[90]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_num.loc[90]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fakedata = [ -118.51 , 34.26 , 29. , 2119.5 , 433. , 1164. , 408. , 3.5409, \"1H OCEAN\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.loc[90] = fakedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.loc[90]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testsillydata = load_housing_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testsillydata.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sillytest_num = testsillydata.drop(\"ocean_proximity\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = imputer.transform(sillytest_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sillytest_num.drop(\"ocean_proximity\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer.fit(sillytest_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sillytest_num.loc[20640]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sillytest_num.loc[20639]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testsillydata = load_housing_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sillytest_num.loc[20639]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testsillydata.loc[20639]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testsillydata_num = testsillydata.drop(\"ocean_proximity\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer.fit(testsillydata_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = imputer.transform(testsillydata_num)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testsillydata_num.loc[20639]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testsillydata[20639]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testsillydata.loc[20639]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dummy_data(housing_path=HOUSING_PATH):\n",
    "    csv_path = os.path.join(housing_path, \"housing_copy_for_exploring.csv\")\n",
    "    return pd.read_csv(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy = load_dummy_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy.loc[7098]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy.loc[7099]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy.loc[7097]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_num = dummy.drop(\"ocean_proximity\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer.fit(dummy_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = imputer.transform(dummy_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy = pd.DataFrame( X, columns = dummy_num.columns, index = housing_num.index)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy = pd.DataFrame( X, columns = dummy_num.columns, index = dummy_num.index)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy.loc[7097]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy.loc[0\n",
    "         ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_cat = housing[\"ocean_proximity\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_cat.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "ordinal_encoder = OrdinalEncoder()\n",
    "\n",
    "housing_cat_encoded = ordinal_encoder.fit_transform(housing_cat)\n",
    "\n",
    "housing_cat_encoded[:10]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "ordinal_encoder = OrdinalEncoder()\n",
    "\n",
    "housing_cat_encoded = ordinal_encoder.fit_transform(housing_cat)\n",
    "\n",
    "housing_cat_encoded[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "ordinal_encoder = OrdinalEncoder()\n",
    "\n",
    "housing_cat_encoded = ordinal_encoder.fit_transform(housing_cat)\n",
    "\n",
    "print(housing_cat_encoded[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "housing_cat = housing[\"ocean_proximity\"]\n",
    "housing_cat_encoded = encoder.fit_transform(housing_cat)\n",
    "housing_cat_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "ordinal_encoder = OrdinalEncoder()\n",
    "\n",
    "housing_cat_encoded = ordinal_encoder.fit_transform(housing_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "ordinal_encoder = OrdinalEncoder()\n",
    "\n",
    "housing_cat = housing[\"ocean_proximity\"]\n",
    "\n",
    "housing_cat_encoded = ordinal_encoder.fit_transform(housing_cat)\n",
    "\n",
    "print(housing_cat_encoded[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "ordinal_encoder = OrdinalEncoder()\n",
    "\n",
    "housing_cat = housing[\"ocean_proximity\"]\n",
    "\n",
    "housing_cat_encoded = ordinal_encoder.fit_transform(housing_cat)\n",
    "\n",
    "print(housing_cat_encoded[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "ordinal_encoder = OrdinalEncoder()\n",
    "\n",
    "housing_cat_encoded = ordinal_encoder.fit_transform(housing_cat)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "ordinal_encoder = OrdinalEncoder()\n",
    "\n",
    "housing_cat = housing[\"ocean_proximity\"]\n",
    "\n",
    "housing_cat.reshape(-1, 1)\n",
    "\n",
    "housing_cat_encoded = ordinal_encoder.fit_transform(housing_cat)\n",
    "\n",
    "\n",
    "\n",
    "print(housing_cat_encoded[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "ordinal_encoder = OrdinalEncoder()\n",
    "\n",
    "housing_cat = housing[\"ocean_proximity\"]\n",
    "\n",
    "\n",
    "\n",
    "housing_cat_encoded = ordinal_encoder.fit_transform(housing_cat).reshape(-1,1)\n",
    "\n",
    "\n",
    "\n",
    "print(housing_cat_encoded[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "ordinal_encoder = OrdinalEncoder()\n",
    "\n",
    "housing_cat = housing[\"ocean_proximity\"]\n",
    "\n",
    "housing_cat_encoded = ordinal_encoder.fit_transform(housing_cat).reshape(-1,1).toarray()\n",
    "\n",
    "print(housing_cat_encoded[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "ordinal_encoder = OrdinalEncoder()\n",
    "\n",
    "housing_cat = housing[\"ocean_proximity\"].toarray()\n",
    "\n",
    "\n",
    "\n",
    "housing_cat_encoded = ordinal_encoder.fit_transform(housing_cat).reshape(-1,1)\n",
    "\n",
    "\n",
    "\n",
    "print(housing_cat_encoded[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "ordinal_encoder = OrdinalEncoder()\n",
    "\n",
    "housing_cat = housing[\"ocean_proximity\"].asmatrix()\n",
    "\n",
    "\n",
    "\n",
    "housing_cat_encoded = ordinal_encoder.fit_transform(housing_cat).reshape(-1,1)\n",
    "\n",
    "\n",
    "\n",
    "print(housing_cat_encoded[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "ordinal_encoder = OrdinalEncoder()\n",
    "\n",
    "housing_cat = housing[[\"ocean_proximity\"]]\n",
    "\n",
    "\n",
    "\n",
    "housing_cat_encoded = ordinal_encoder.fit_transform(housing_cat)\n",
    "\n",
    "\n",
    "\n",
    "print(housing_cat_encoded[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "ordinal_encoder = OrdinalEncoder()\n",
    "\n",
    "housing_cat = housing[[\"ocean_proximity\"]]\n",
    "\n",
    "\n",
    "\n",
    "housing_cat_encoded = ordinal_encoder.fit_transform(housing_cat)\n",
    "\n",
    "\n",
    "\n",
    "print(housing_cat_encoded[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "ordinal_encoder = OrdinalEncoder()\n",
    "\n",
    "housing_cat = housing[[\"ocean_proximity\"]]\n",
    "\n",
    "\n",
    "\n",
    "housing_cat_encoded = ordinal_encoder.fit_transform(housing_cat)\n",
    "\n",
    "\n",
    "\n",
    "print(housing_cat_encoded[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "from six.moves import urllib\n",
    "\n",
    "HOUSING_PATH = \"datasets/housing\"\n",
    "HOUSING_URL = DATASETS_URL + \"/housing/housing.tgz\"\n",
    "\n",
    "def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):\n",
    "    if not os.path.exists(housing_path):\n",
    "        os.makedirs(housing_path)\n",
    "    tgz_path = os.path.join(housing_path, \"housing.tgz\")\n",
    "    urllib.request.urlretrieve(housing_url, tgz_path)\n",
    "    housing_tgz = tarfile.open(tgz_path)\n",
    "    housing_tgz.extractall(path=housing_path)\n",
    "    housing_tgz.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16512 4128\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_housing_data(housing_path=HOUSING_PATH):\n",
    "    csv_path = os.path.join(housing_path, \"housing.csv\")\n",
    "    return pd.read_csv(csv_path)\n",
    "\n",
    "housing = load_housing_data()\n",
    "housing.head()\n",
    "\n",
    "import numpy as np\n",
    "import numpy.random as rnd\n",
    "rnd.seed(42) # to make this notebook's output identical at every run\n",
    "\n",
    "def split_train_test(data, test_ratio):\n",
    "    shuffled_indices = rnd.permutation(len(data))\n",
    "    test_set_size = int(len(data) * test_ratio)\n",
    "    test_indices = shuffled_indices[:test_set_size]\n",
    "    train_indices = shuffled_indices[test_set_size:]\n",
    "    return data.iloc[train_indices], data.iloc[test_indices]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_set, test_set = split_train_test(housing, 0.2)\n",
    "print(len(train_set), len(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "\n",
    "def test_set_check(identifier, test_ratio, hash):\n",
    "    return bytearray(hash(np.int64(identifier)).digest())[-1] < 256 * test_ratio\n",
    "\n",
    "def split_train_test_by_id(data, test_ratio, id_column, hash=hashlib.md5):\n",
    "    ids = data[id_column]\n",
    "    in_test_set = ids.apply(lambda id_: test_set_check(id_, test_ratio, hash))\n",
    "    return data.loc[~in_test_set], data.loc[in_test_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>housing_median_age</th>\n",
       "      <th>total_rooms</th>\n",
       "      <th>total_bedrooms</th>\n",
       "      <th>population</th>\n",
       "      <th>households</th>\n",
       "      <th>median_income</th>\n",
       "      <th>median_house_value</th>\n",
       "      <th>ocean_proximity</th>\n",
       "      <th>income_cat</th>\n",
       "      <th>rooms_per_household</th>\n",
       "      <th>bedrooms_per_room</th>\n",
       "      <th>population_per_household</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>-122.25</td>\n",
       "      <td>37.85</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1627.0</td>\n",
       "      <td>280.0</td>\n",
       "      <td>565.0</td>\n",
       "      <td>259.0</td>\n",
       "      <td>3.8462</td>\n",
       "      <td>342200.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.879646</td>\n",
       "      <td>0.172096</td>\n",
       "      <td>2.181467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>-122.25</td>\n",
       "      <td>37.85</td>\n",
       "      <td>52.0</td>\n",
       "      <td>919.0</td>\n",
       "      <td>213.0</td>\n",
       "      <td>413.0</td>\n",
       "      <td>193.0</td>\n",
       "      <td>4.0368</td>\n",
       "      <td>269700.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.225182</td>\n",
       "      <td>0.231774</td>\n",
       "      <td>2.139896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>-122.26</td>\n",
       "      <td>37.85</td>\n",
       "      <td>52.0</td>\n",
       "      <td>3503.0</td>\n",
       "      <td>752.0</td>\n",
       "      <td>1504.0</td>\n",
       "      <td>734.0</td>\n",
       "      <td>3.2705</td>\n",
       "      <td>241800.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.329122</td>\n",
       "      <td>0.214673</td>\n",
       "      <td>2.049046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>-122.27</td>\n",
       "      <td>37.85</td>\n",
       "      <td>40.0</td>\n",
       "      <td>751.0</td>\n",
       "      <td>184.0</td>\n",
       "      <td>409.0</td>\n",
       "      <td>166.0</td>\n",
       "      <td>1.3578</td>\n",
       "      <td>147500.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.836186</td>\n",
       "      <td>0.245007</td>\n",
       "      <td>2.463855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>-122.27</td>\n",
       "      <td>37.84</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1688.0</td>\n",
       "      <td>337.0</td>\n",
       "      <td>853.0</td>\n",
       "      <td>325.0</td>\n",
       "      <td>2.1806</td>\n",
       "      <td>99700.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.978898</td>\n",
       "      <td>0.199645</td>\n",
       "      <td>2.624615</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    index  longitude  latitude  housing_median_age  total_rooms  \\\n",
       "4       4    -122.25     37.85                52.0       1627.0   \n",
       "5       5    -122.25     37.85                52.0        919.0   \n",
       "11     11    -122.26     37.85                52.0       3503.0   \n",
       "20     20    -122.27     37.85                40.0        751.0   \n",
       "23     23    -122.27     37.84                52.0       1688.0   \n",
       "\n",
       "    total_bedrooms  population  households  median_income  median_house_value  \\\n",
       "4            280.0       565.0       259.0         3.8462            342200.0   \n",
       "5            213.0       413.0       193.0         4.0368            269700.0   \n",
       "11           752.0      1504.0       734.0         3.2705            241800.0   \n",
       "20           184.0       409.0       166.0         1.3578            147500.0   \n",
       "23           337.0       853.0       325.0         2.1806             99700.0   \n",
       "\n",
       "   ocean_proximity  income_cat  rooms_per_household  bedrooms_per_room  \\\n",
       "4         NEAR BAY         3.0             2.879646           0.172096   \n",
       "5         NEAR BAY         3.0             2.225182           0.231774   \n",
       "11        NEAR BAY         3.0             2.329122           0.214673   \n",
       "20        NEAR BAY         1.0             1.836186           0.245007   \n",
       "23        NEAR BAY         2.0             1.978898           0.199645   \n",
       "\n",
       "    population_per_household  \n",
       "4                   2.181467  \n",
       "5                   2.139896  \n",
       "11                  2.049046  \n",
       "20                  2.463855  \n",
       "23                  2.624615  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing_with_id = housing.reset_index()   # adds an `index` column\n",
    "train_set, test_set = split_train_test_by_id(housing_with_id, 0.2, \"index\")\n",
    "test_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>housing_median_age</th>\n",
       "      <th>total_rooms</th>\n",
       "      <th>total_bedrooms</th>\n",
       "      <th>population</th>\n",
       "      <th>households</th>\n",
       "      <th>median_income</th>\n",
       "      <th>median_house_value</th>\n",
       "      <th>ocean_proximity</th>\n",
       "      <th>income_cat</th>\n",
       "      <th>rooms_per_household</th>\n",
       "      <th>bedrooms_per_room</th>\n",
       "      <th>population_per_household</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20046</th>\n",
       "      <td>-119.01</td>\n",
       "      <td>36.06</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1505.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1392.0</td>\n",
       "      <td>359.0</td>\n",
       "      <td>1.6812</td>\n",
       "      <td>47700.0</td>\n",
       "      <td>INLAND</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.081178</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.877437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3024</th>\n",
       "      <td>-119.46</td>\n",
       "      <td>35.14</td>\n",
       "      <td>30.0</td>\n",
       "      <td>2943.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1565.0</td>\n",
       "      <td>584.0</td>\n",
       "      <td>2.5313</td>\n",
       "      <td>45800.0</td>\n",
       "      <td>INLAND</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.880511</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.679795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15663</th>\n",
       "      <td>-122.44</td>\n",
       "      <td>37.80</td>\n",
       "      <td>52.0</td>\n",
       "      <td>3830.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1310.0</td>\n",
       "      <td>963.0</td>\n",
       "      <td>3.4801</td>\n",
       "      <td>500001.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.923664</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.360332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20484</th>\n",
       "      <td>-118.72</td>\n",
       "      <td>34.28</td>\n",
       "      <td>17.0</td>\n",
       "      <td>3051.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1705.0</td>\n",
       "      <td>495.0</td>\n",
       "      <td>5.7376</td>\n",
       "      <td>218600.0</td>\n",
       "      <td>&lt;1H OCEAN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.789443</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.444444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9814</th>\n",
       "      <td>-121.93</td>\n",
       "      <td>36.62</td>\n",
       "      <td>34.0</td>\n",
       "      <td>2351.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1063.0</td>\n",
       "      <td>428.0</td>\n",
       "      <td>3.7250</td>\n",
       "      <td>278000.0</td>\n",
       "      <td>NEAR OCEAN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.211665</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.483645</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\n",
       "20046    -119.01     36.06                25.0       1505.0             NaN   \n",
       "3024     -119.46     35.14                30.0       2943.0             NaN   \n",
       "15663    -122.44     37.80                52.0       3830.0             NaN   \n",
       "20484    -118.72     34.28                17.0       3051.0             NaN   \n",
       "9814     -121.93     36.62                34.0       2351.0             NaN   \n",
       "\n",
       "       population  households  median_income  median_house_value  \\\n",
       "20046      1392.0       359.0         1.6812             47700.0   \n",
       "3024       1565.0       584.0         2.5313             45800.0   \n",
       "15663      1310.0       963.0         3.4801            500001.0   \n",
       "20484      1705.0       495.0         5.7376            218600.0   \n",
       "9814       1063.0       428.0         3.7250            278000.0   \n",
       "\n",
       "      ocean_proximity  income_cat  rooms_per_household  bedrooms_per_room  \\\n",
       "20046          INLAND         2.0             1.081178                NaN   \n",
       "3024           INLAND         2.0             1.880511                NaN   \n",
       "15663        NEAR BAY         3.0             2.923664                NaN   \n",
       "20484       <1H OCEAN         4.0             1.789443                NaN   \n",
       "9814       NEAR OCEAN         3.0             2.211665                NaN   \n",
       "\n",
       "       population_per_household  \n",
       "20046                  3.877437  \n",
       "3024                   2.679795  \n",
       "15663                  1.360332  \n",
       "20484                  3.444444  \n",
       "9814                   2.483645  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)\n",
    "test_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0    7236\n",
       "2.0    6581\n",
       "4.0    3639\n",
       "5.0    2362\n",
       "1.0     822\n",
       "Name: income_cat, dtype: int64"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing[\"income_cat\"] = np.ceil(housing[\"median_income\"] / 1.5)\n",
    "housing[\"income_cat\"].where(housing[\"income_cat\"] < 5, 5.0, inplace=True)\n",
    "housing[\"income_cat\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "for train_index, test_index in split.split(housing, housing[\"income_cat\"]):\n",
    "    strat_train_set = housing.loc[train_index]\n",
    "    strat_test_set = housing.loc[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def income_cat_proportions(data):\n",
    "    return data[\"income_cat\"].value_counts() / len(data)\n",
    "\n",
    "train_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)\n",
    "\n",
    "compare_props = pd.DataFrame({\n",
    "    \"Overall\": income_cat_proportions(housing),\n",
    "    \"Stratified\": income_cat_proportions(strat_test_set),\n",
    "    \"Random\": income_cat_proportions(test_set),\n",
    "}).sort_index()\n",
    "compare_props[\"Rand. %error\"] = 100 * compare_props[\"Random\"] / compare_props[\"Overall\"] - 100\n",
    "compare_props[\"Strat. %error\"] = 100 * compare_props[\"Stratified\"] / compare_props[\"Overall\"] - 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Overall</th>\n",
       "      <th>Stratified</th>\n",
       "      <th>Random</th>\n",
       "      <th>Rand. %error</th>\n",
       "      <th>Strat. %error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>0.039826</td>\n",
       "      <td>0.039729</td>\n",
       "      <td>0.040213</td>\n",
       "      <td>0.973236</td>\n",
       "      <td>-0.243309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>0.318847</td>\n",
       "      <td>0.318798</td>\n",
       "      <td>0.324370</td>\n",
       "      <td>1.732260</td>\n",
       "      <td>-0.015195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.0</th>\n",
       "      <td>0.350581</td>\n",
       "      <td>0.350533</td>\n",
       "      <td>0.358527</td>\n",
       "      <td>2.266446</td>\n",
       "      <td>-0.013820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.0</th>\n",
       "      <td>0.176308</td>\n",
       "      <td>0.176357</td>\n",
       "      <td>0.167393</td>\n",
       "      <td>-5.056334</td>\n",
       "      <td>0.027480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5.0</th>\n",
       "      <td>0.114438</td>\n",
       "      <td>0.114583</td>\n",
       "      <td>0.109496</td>\n",
       "      <td>-4.318374</td>\n",
       "      <td>0.127011</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Overall  Stratified    Random  Rand. %error  Strat. %error\n",
       "1.0  0.039826    0.039729  0.040213      0.973236      -0.243309\n",
       "2.0  0.318847    0.318798  0.324370      1.732260      -0.015195\n",
       "3.0  0.350581    0.350533  0.358527      2.266446      -0.013820\n",
       "4.0  0.176308    0.176357  0.167393     -5.056334       0.027480\n",
       "5.0  0.114438    0.114583  0.109496     -4.318374       0.127011"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_props"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "for set in (strat_train_set, strat_test_set):\n",
    "    set.drop(\"income_cat\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = strat_train_set.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "median_house_value          1.000000\n",
       "median_income               0.687160\n",
       "rooms_per_household         0.199429\n",
       "total_rooms                 0.135097\n",
       "housing_median_age          0.114110\n",
       "households                  0.064506\n",
       "total_bedrooms              0.047689\n",
       "population_per_household   -0.021985\n",
       "population                 -0.026920\n",
       "longitude                  -0.047432\n",
       "latitude                   -0.142724\n",
       "bedrooms_per_room          -0.259984\n",
       "Name: median_house_value, dtype: float64"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr_matrix = housing.corr()\n",
    "corr_matrix[\"median_house_value\"].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing[\"rooms_per_household\"] = housing[\"total_rooms\"] / housing[\"population\"]\n",
    "housing[\"bedrooms_per_room\"] = housing[\"total_bedrooms\"] / housing[\"total_rooms\"]\n",
    "housing[\"population_per_household\"] = housing[\"population\"] / housing[\"households\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "median_house_value          1.000000\n",
       "median_income               0.687160\n",
       "rooms_per_household         0.199429\n",
       "total_rooms                 0.135097\n",
       "housing_median_age          0.114110\n",
       "households                  0.064506\n",
       "total_bedrooms              0.047689\n",
       "population_per_household   -0.021985\n",
       "population                 -0.026920\n",
       "longitude                  -0.047432\n",
       "latitude                   -0.142724\n",
       "bedrooms_per_room          -0.259984\n",
       "Name: median_house_value, dtype: float64"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr_matrix = housing.corr()\n",
    "corr_matrix[\"median_house_value\"].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "housing = strat_train_set.drop(\"median_house_value\", axis=1)\n",
    "housing_labels = strat_train_set[\"median_house_value\"].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 16512 entries, 17606 to 15775\n",
      "Data columns (total 11 columns):\n",
      " #   Column                    Non-Null Count  Dtype  \n",
      "---  ------                    --------------  -----  \n",
      " 0   longitude                 16512 non-null  float64\n",
      " 1   latitude                  16512 non-null  float64\n",
      " 2   housing_median_age        16512 non-null  float64\n",
      " 3   total_rooms               16512 non-null  float64\n",
      " 4   total_bedrooms            16512 non-null  float64\n",
      " 5   population                16512 non-null  float64\n",
      " 6   households                16512 non-null  float64\n",
      " 7   median_income             16512 non-null  float64\n",
      " 8   rooms_per_household       16512 non-null  float64\n",
      " 9   bedrooms_per_room         16512 non-null  float64\n",
      " 10  population_per_household  16512 non-null  float64\n",
      "dtypes: float64(11)\n",
      "memory usage: 1.5 MB\n"
     ]
    }
   ],
   "source": [
    "median = housing[\"total_bedrooms\"]. median()\n",
    "\n",
    "from sklearn.impute import SimpleImputer \n",
    "imputer = SimpleImputer( strategy =\"median\")\n",
    "\n",
    "\n",
    "housing_num = housing.drop(\"ocean_proximity\", axis = 1)\n",
    "\n",
    "imputer.fit(housing_num)\n",
    "\n",
    "X = imputer.transform(housing_num)\n",
    "\n",
    "housing_tr = pd.DataFrame(X, columns = housing_num.columns, index = housing_num.index)\n",
    "\n",
    "housing_tr.info()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ocean_proximity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17606</th>\n",
       "      <td>&lt;1H OCEAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18632</th>\n",
       "      <td>&lt;1H OCEAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14650</th>\n",
       "      <td>NEAR OCEAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3230</th>\n",
       "      <td>INLAND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3555</th>\n",
       "      <td>&lt;1H OCEAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19480</th>\n",
       "      <td>INLAND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8879</th>\n",
       "      <td>&lt;1H OCEAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13685</th>\n",
       "      <td>INLAND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4937</th>\n",
       "      <td>&lt;1H OCEAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4861</th>\n",
       "      <td>&lt;1H OCEAN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ocean_proximity\n",
       "17606       <1H OCEAN\n",
       "18632       <1H OCEAN\n",
       "14650      NEAR OCEAN\n",
       "3230           INLAND\n",
       "3555        <1H OCEAN\n",
       "19480          INLAND\n",
       "8879        <1H OCEAN\n",
       "13685          INLAND\n",
       "4937        <1H OCEAN\n",
       "4861        <1H OCEAN"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing_cat = housing[[\"ocean_proximity\"]]\n",
    "\n",
    "housing_cat.head(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.],\n",
       "       [0.],\n",
       "       [4.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder \n",
    "\n",
    "ordinal_encoder = OrdinalEncoder() \n",
    "housing_cat_encoded = ordinal_encoder.fit_transform(housing_cat) \n",
    "housing_cat_encoded[: 10]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['<1H OCEAN', 'INLAND', 'ISLAND', 'NEAR BAY', 'NEAR OCEAN'],\n",
       "       dtype=object)]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ordinal_encoder.categories_\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<16512x5 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 16512 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "cat_encoder = OneHotEncoder()\n",
    "housing_cat_1hot = cat_encoder.fit_transform(housing_cat) \n",
    "housing_cat_1hot\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       ...,\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0.]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing_cat_1hot.toarray()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the one hot array above does the following,\n",
    "# it lines up the text categories left to right symbolically\n",
    "# a 1 is placed in the column representative of the actual text for \n",
    "# that row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#As written in the text, creating the hyper parameter add bedrooms, we are kind of testing\n",
    "#the hyper parameter.. Is it good? well we can first flag it true and test,\n",
    "#then flag it false and test\n",
    "# In this example the transformer has one hyperparameter, add_bedrooms_per_room, \n",
    "# set to True by default (it is often helpful to provide sensible defaults). \n",
    "# This hyperparameter will allow you to easily find out whether adding this\n",
    "# attribute helps the Machine Learning algorithms or not. More generally,\n",
    "# you can add a hyperparameter to gate any data preparation step that you\n",
    "# are not 100% sure about. The more you automate these data preparation steps, \n",
    "# the more combinations you can automatically try out, making it much more likely\n",
    "# that you will find a great combination (and saving you a lot of time).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#for custom transformers three things needed\n",
    "#fit(), transform(), and fit_transform()\n",
    "#note you see 2 below, the third fit_transform()\n",
    "#is included by adding \"TransformerMixin\" as a base class\n",
    "#the \"BaseEstimator\" base class gives two free methods\n",
    "#get_params() and set_params() which are good for auto tuning\n",
    "#hyper params such a hyper param might be the stategy type\n",
    "#of median for the \"SimpleImputer\"\n",
    "from sklearn.base import BaseEstimator, TransformerMixin \n",
    "\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "#these are column indices for the dataset\n",
    "rooms_ix, bedrooms_ix, population_ix, households_ix = 3, 4, 5, 6 \n",
    "\n",
    "class CombinedAttributesAdder(BaseEstimator, TransformerMixin): \n",
    "    \n",
    "    # no *args or ** kargs\n",
    "    #custom transformer to add the custom attribute \n",
    "    #bedrooms_per_room which combines total_bedrooms/total_rooms columns\n",
    "    def __init__(self, add_bedrooms_per_room = True): \n",
    "        #set boolean true?\n",
    "        self.add_bedrooms_per_room = add_bedrooms_per_room \n",
    "    def fit( self, X, y = None): \n",
    "            # nothing else to do\n",
    "            return self  \n",
    "    def transform(self, X): \n",
    "            rooms_per_household = X[:, rooms_ix] / X[:, households_ix] \n",
    "            population_per_household = X[:, population_ix] / X[:, households_ix] \n",
    "            #note here is what np.c_ does\n",
    "            #np.c_[np.array([1,2,3]), np.array([4,5,6])]\n",
    "            #array([[1, 4],\n",
    "            #[2, 5],\n",
    "            #[3, 6]])\n",
    "            #combines columns in input array into 2d array\n",
    "            if self.add_bedrooms_per_room: \n",
    "                bedrooms_per_room = X[:, bedrooms_ix] / X[:, rooms_ix] \n",
    "                \n",
    "                returnVal = np.c_[X, rooms_per_household, population_per_household, bedrooms_per_room] \n",
    "                \n",
    "                sel = VarianceThreshold(threshold=(.8 * (1 - .8)))\n",
    "                sel.fit_transform(returnVal)\n",
    "                \n",
    "                return returnVal\n",
    "            else: \n",
    "                return np.c_[X, rooms_per_household, population_per_household] \n",
    "            \n",
    "            \n",
    "            attr_adder = CombinedAttributesAdder( add_bedrooms_per_room = False) \n",
    "            housing_extra_attribs = attr_adder.transform( housing.values)\n",
    "\n",
    "    \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline \n",
    "from sklearn.preprocessing import StandardScaler \n",
    "\n",
    "#set the sequence of transformations\n",
    "num_pipeline = Pipeline([ ('imputer', SimpleImputer( strategy =\"median\")), \n",
    "                         ('attribs_adder', CombinedAttributesAdder()), \n",
    "                         ('std_scaler', StandardScaler()), ]) \n",
    "#apply the sequence\n",
    "housing_num_tr = num_pipeline.fit_transform(housing_num)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline \n",
    "from sklearn.preprocessing import StandardScaler \n",
    "\n",
    "#set the sequence of transformations\n",
    "#pair of name estimator pairs\n",
    "#all but last must be transformer\n",
    "num_pipeline = Pipeline([ ('imputer', SimpleImputer( strategy =\"median\")), \n",
    "                         ('attribs_adder', CombinedAttributesAdder()), \n",
    "                         ('std_scaler', StandardScaler()), ]) \n",
    "#apply the sequence, calls fit_transform() sequentially for each pair above\n",
    "#output of first pair passed to second pair, etc..\n",
    "#final estimator calls fit()\n",
    "housing_num_tr = num_pipeline.fit_transform(housing_num)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#this cell takes the \"ocean_proximity\" column and transforms it\n",
    "#to be numerical like the rest of our columns,\n",
    "#it does so using pipeline\n",
    "#we use \"ColumnTransformer\" as well\n",
    "from sklearn.compose import ColumnTransformer \n",
    "\n",
    "#get numerical columns\n",
    "num_attribs = list( housing_num) \n",
    "#get text columns\n",
    "cat_attribs = [\"ocean_proximity\"] \n",
    "\n",
    "#apply numerical pipelines to num columns portion of data\n",
    "#apply one hot to category column\n",
    "full_pipeline = ColumnTransformer([ (\"num\", num_pipeline, num_attribs), \n",
    "                                   (\"cat\", OneHotEncoder(), cat_attribs),]) \n",
    "\n",
    "#apply the full pipeline to all of the data\n",
    "housing_prepared = full_pipeline.fit_transform( housing)\n",
    "\n",
    "\n",
    "#SMALL NOTE\n",
    "#Note that the OneHotEncoder returns a sparse matrix, while the\n",
    "# num_pipeline returns a dense matrix. When there is such a mix of sparse\n",
    "# and dense matrices, the ColumnTransformer estimates the density of the final\n",
    "# matrix (i.e., the ratio of nonzero cells), and it returns a sparse matrix\n",
    "# if the density is lower than a given threshold (by default, sparse_threshold = 0.3).\n",
    "# In this example, it returns a dense matrix. And thatâ€™s it! We have a preprocessing\n",
    "# pipeline that takes the full housing data and applies the appropriate transformations to\n",
    "# each column.\n",
    "\n",
    "#essentially we don't need to worry about this\n",
    "\n",
    "#Note sparse matrix has mainly zeros whereas dense matrix has few zeros and mainly non zeros\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TIP\n",
    "# Instead of using a transformer, you can specify the \n",
    "# string \"drop\" if you want the columns to be dropped,\n",
    "# or you can specify \"passthrough\" if you want the columns\n",
    "# to be left untouched. By default, the remaining columns\n",
    "# (i.e., the ones that were not listed) will be dropped, but \n",
    "# you can set the remainder hyperparameter to any transformer \n",
    "# (or to \"passthrough\") if you want these columns to be handled differently.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit( housing_prepared, housing_labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [220977.   320386.25 218335.75  47149.5  185789.25]\n"
     ]
    }
   ],
   "source": [
    "#these are our x inputs\n",
    "some_data = housing.iloc[: 5]\n",
    "#these are our labels (actual outputs to compare agains)\n",
    "some_labels = housing_labels.iloc[: 5]\n",
    "#apply our transformations to the same inputs of the training set\n",
    "some_data_prepared = full_pipeline.transform( some_data)\n",
    "\n",
    "#use linear regression to predict outputs based on inputs\n",
    "print(\"Predictions:\", lin_reg.predict( some_data_prepared))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels: [286600.0, 340600.0, 196900.0, 46300.0, 254500.0]\n"
     ]
    }
   ],
   "source": [
    "#print the actual data\n",
    "print(\"Labels:\", list( some_labels))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the predictions from the linear regression are not that far off..\n",
    "#well not too far at least.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#measure the error using RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66988.5670034171"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error \n",
    "\n",
    "housing_predictions = lin_reg.predict(housing_prepared) \n",
    "lin_mse = mean_squared_error( housing_labels, housing_predictions) \n",
    "lin_rmse = np.sqrt( lin_mse) \n",
    "lin_rmse\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#underfitting according to RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "#time to use a more complex model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Letâ€™s train a DecisionTreeRegressor. This is a powerful model,\n",
    "# capable of finding complex nonlinear relationships in the data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor()"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor \n",
    "\n",
    "tree_reg = DecisionTreeRegressor() \n",
    "#housing prepared is the dataset after going through full pipeline\n",
    "#housing labels are the target values\n",
    "tree_reg.fit(housing_prepared, housing_labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calc predictions and RMSE \n",
    "housing_predictions = tree_reg.predict(housing_prepared) \n",
    "tree_mse = mean_squared_error(housing_labels, housing_predictions) \n",
    "tree_rmse = np.sqrt( tree_mse) \n",
    "tree_rmse\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "#^0 error.... when using DecisionTreeRegressor()..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "#good, but we are likely overfitting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we should NOT test the test set as this overfitting will not adapt well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Better Evaluation Using Cross-Validation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following code randomly splits the training set into 10 distinct subsets called folds,\n",
    "# then it trains and evaluates the Decision Tree model 10 times, picking a different\n",
    "# fold for evaluation every time and training on the other 9 folds. The result is an\n",
    "# array containing the 10 evaluation scores:\n",
    "\n",
    "    \n",
    "from sklearn.model_selection import cross_val_score \n",
    "\n",
    "#use tree reg with data through transformation pipeline, target vals, and score using neg RMSE#\n",
    "scores = cross_val_score(tree_reg, housing_prepared, housing_labels, scoring =\"neg_mean_squared_error\", cv = 10) \n",
    "tree_rmse_scores = np.sqrt(-scores)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "#^ for SciKit Learn's validation greater is better versus our typical cost of lower is better\n",
    "#hence the negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_scores( scores):\n",
    "    print(\"Scores:\", scores) \n",
    "    print(\"Mean:\", scores.mean())\n",
    "    print(\"Standard deviation:\", scores.std())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: [70059.24367977 67386.94311469 72494.33219754 69079.49366743\n",
      " 71332.88518335 72434.88942976 69872.16608651 67565.07065536\n",
      " 74851.60865038 70011.26009148]\n",
      "Mean: 70508.78927562766\n",
      "Standard deviation: 2202.2865618624623\n"
     ]
    }
   ],
   "source": [
    "display_scores( tree_rmse_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "#overall the evaluation scores per cross validation are worse than the linear regression model...\n",
    "#this is good because cross validation tests generalization by using the folds method, so it shows even within \n",
    "#these tiny folds in the training set (where we know the issue is not bad data so we don't need train dev sest)\n",
    "#the data is overfitted since it can't even adapt to new values in the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to confirm if the tree is performing worse, let us use \n",
    "#cross validation on the linear regression algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: [64390.24134807 65974.69195641 66487.23412236 70607.52685338\n",
      " 66391.15346493 69595.7874953  70436.43797153 66982.6239614\n",
      " 71304.30596222 65717.0675605 ]\n",
      "Mean: 67788.70706960869\n",
      "Standard deviation: 2324.0023101500465\n"
     ]
    }
   ],
   "source": [
    "lin_scores = cross_val_score(lin_reg, housing_prepared, housing_labels, \n",
    "                             scoring =\"neg_mean_squared_error\", \n",
    "                             cv = 10) \n",
    "lin_rmse_scores = np.sqrt(-lin_scores) \n",
    "display_scores( lin_rmse_scores)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Yep.. on average the linear regression algorithm performs better...\n",
    "#the tree is critically overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we try RandomForestRegressor which trains many decision trees on random subsets of\n",
    "#the features (attributes)\n",
    "#Since we are building a model (RandomForestRegressor)\n",
    "#on top of other models (DecisionTrees)\n",
    "#this is called EnsembleLearning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18412.861997137785"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor \n",
    "\n",
    "forest_reg = RandomForestRegressor() \n",
    "forest_reg.fit( housing_prepared, housing_labels)\n",
    "\n",
    "housing_predictions = forest_reg.predict(housing_prepared) \n",
    "forest_mse = mean_squared_error(housing_labels, housing_predictions) \n",
    "forest_rmse = np.sqrt(forest_mse) \n",
    "\n",
    "forest_rmse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: [49119.50960336 46747.26348778 48862.65184201 51689.28710937\n",
      " 48361.19456053 52937.85565992 47748.01360455 47585.60573973\n",
      " 52218.09565989 49118.48547668]\n",
      "Mean: 49438.796274380606\n",
      "Standard deviation: 2006.6826215893134\n"
     ]
    }
   ],
   "source": [
    "forest_scores = cross_val_score(forest_reg, housing_prepared, housing_labels, \n",
    "                             scoring =\"neg_mean_squared_error\", \n",
    "                             cv = 10) \n",
    "forest_rmse_scores = np.sqrt(-forest_scores) \n",
    "display_scores( forest_rmse_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save our models for later review using Joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import joblib \n",
    "#joblib.dump( my_model, \"my_model.pkl\") \n",
    "# and later...\n",
    "#my_model_loaded = joblib.load(\"my_model.pkl\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing SVM\n",
    "from sklearn import svm\n",
    "\n",
    "\n",
    "\n",
    "svm_reg = svm.SVR()\n",
    "svm_reg.fit(housing_prepared, housing_labels)\n",
    "\n",
    "\n",
    "housing_predictions = svm_reg.predict(housing_prepared) \n",
    "svm_mse = mean_squared_error(housing_labels, housing_predictions) \n",
    "svm_rmse = np.sqrt(svm_mse) \n",
    "\n",
    "svm_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_scores = cross_val_score(svm_reg, housing_prepared, housing_labels, \n",
    "                             scoring =\"neg_mean_squared_error\", \n",
    "                             cv = 10) \n",
    "svm_rmse_scores = np.sqrt(-svm_scores) \n",
    "display_scores(svm_rmse_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 31, 'max_features': 12}"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fine tune the hyperparameters of random forest using GridSearchCV\n",
    "from sklearn.model_selection import GridSearchCV \n",
    "\n",
    "#try using 3, 10, and 30 estimators, 2, 4, 6, and 8 max features\n",
    "#false the bootstrap(some resampling technique)\n",
    "#note we have two arrays in here each with their tweaks\n",
    "# param_grid = [ {'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]}, \n",
    "#               {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]}, ]\n",
    "\n",
    "forest_reg = RandomForestRegressor() \n",
    "\n",
    "#note cv = 5 means five fold cross validation\n",
    "#previously we were using 10 fold\n",
    "# grid_search = GridSearchCV( forest_reg, param_grid, cv = 5, \n",
    "#                            scoring ='neg_mean_squared_error', \n",
    "#                            return_train_score = True) \n",
    "# grid_search.fit( housing_prepared, housing_labels)\n",
    "    \n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "distributions = dict({'n_estimators': [3, 10, 15, 17, 18, 30, 31, 32], 'max_features': [2, 4, 6, 8, 10, 12, 14]}, \n",
    "               )\n",
    "\n",
    "\n",
    "clf = RandomizedSearchCV(forest_reg, distributions, random_state=0)\n",
    "search = clf.fit(housing_prepared, housing_labels)\n",
    "search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 30, 'max_features': 12}\n",
      "48343.89508923807\n",
      "{'n_estimators': 10, 'max_features': 10}\n",
      "50433.605128430136\n",
      "{'n_estimators': 3, 'max_features': 12}\n",
      "56122.40834433993\n",
      "{'n_estimators': 15, 'max_features': 8}\n",
      "49549.65867053875\n",
      "{'n_estimators': 17, 'max_features': 4}\n",
      "50732.4436794623\n",
      "{'n_estimators': 15, 'max_features': 2}\n",
      "54151.88616394182\n",
      "{'n_estimators': 3, 'max_features': 10}\n",
      "56888.149049544714\n",
      "{'n_estimators': 17, 'max_features': 12}\n",
      "48772.920656030605\n",
      "{'n_estimators': 31, 'max_features': 12}\n",
      "48982.95324717629\n",
      "{'n_estimators': 31, 'max_features': 8}\n",
      "48768.844375520064\n"
     ]
    }
   ],
   "source": [
    "#testing randoms best RMSE \n",
    "#not so hot...\n",
    "\n",
    "#able to print the results myself\n",
    "cvres = search.cv_results_\n",
    "for mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n",
    "    print(params)\n",
    "    num_estim = params['n_estimators']\n",
    "    max_feat = params['max_features']\n",
    "    final_model = RandomForestRegressor(n_estimators=num_estim, max_features=max_feat).fit(housing_prepared, housing_labels)\n",
    "    X_test = strat_test_set.drop(\"median_house_value\", axis = 1) \n",
    "    y_test = strat_test_set[\"median_house_value\"].copy() \n",
    "    X_test_prepared = full_pipeline.transform( X_test) \n",
    "    final_predictions = final_model.predict(X_test_prepared) \n",
    "    final_mse = mean_squared_error( y_test, final_predictions) \n",
    "    final_rmse = np.sqrt(final_mse)\n",
    "    print(final_rmse)\n",
    "\n",
    "# for i in search.cv_results_:\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "#tip\n",
    "# When you have no idea what value a hyperparameter should have, a simple\n",
    "# approach is to try out consecutive powers of 10 (or a smaller number if\n",
    "# you want a more fine-grained search, as shown in this example with the n_estimators\n",
    "# hyperparameter).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestRegressor(max_features=12, n_estimators=31)\n"
     ]
    }
   ],
   "source": [
    "print(search.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 16512 entries, 17606 to 15775\n",
      "Data columns (total 11 columns):\n",
      " #   Column                    Non-Null Count  Dtype  \n",
      "---  ------                    --------------  -----  \n",
      " 0   longitude                 16512 non-null  float64\n",
      " 1   latitude                  16512 non-null  float64\n",
      " 2   housing_median_age        16512 non-null  float64\n",
      " 3   total_rooms               16512 non-null  float64\n",
      " 4   total_bedrooms            16512 non-null  float64\n",
      " 5   population                16512 non-null  float64\n",
      " 6   households                16512 non-null  float64\n",
      " 7   median_income             16512 non-null  float64\n",
      " 8   rooms_per_household       16512 non-null  float64\n",
      " 9   bedrooms_per_room         16512 non-null  float64\n",
      " 10  population_per_household  16512 non-null  float64\n",
      "dtypes: float64(11)\n",
      "memory usage: 1.5 MB\n"
     ]
    }
   ],
   "source": [
    "median = housing[\"total_bedrooms\"]. median()\n",
    "\n",
    "from sklearn.impute import SimpleImputer \n",
    "imputer = SimpleImputer( strategy =\"median\")\n",
    "\n",
    "\n",
    "housing_num = housing.drop(\"ocean_proximity\", axis = 1)\n",
    "\n",
    "imputer.fit(housing_num)\n",
    "\n",
    "X = imputer.transform(housing_num)\n",
    "\n",
    "housing_num = pd.DataFrame(X, columns = housing_num.columns, index = housing_num.index)\n",
    "\n",
    "housing_num.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "#     X, y = load_iris(return_X_y=True)\n",
    "# >>> X.shape\n",
    "# (150, 4)\n",
    "#     X_new = SelectKBest(chi2, k=2).fit_transform(X, y)\n",
    "#     X_new.shape\n",
    "# (150, 2)\n",
    "\n",
    "\n",
    "#As written in the text, creating the hyper parameter add bedrooms, we are kind of testing\n",
    "#the hyper parameter.. Is it good? well we can first flag it true and test,\n",
    "#then flag it false and test\n",
    "# In this example the transformer has one hyperparameter, add_bedrooms_per_room, \n",
    "# set to True by default (it is often helpful to provide sensible defaults). \n",
    "# This hyperparameter will allow you to easily find out whether adding this\n",
    "# attribute helps the Machine Learning algorithms or not. More generally,\n",
    "# you can add a hyperparameter to gate any data preparation step that you\n",
    "# are not 100% sure about. The more you automate these data preparation steps, \n",
    "# the more combinations you can automatically try out, making it much more likely\n",
    "# that you will find a great combination (and saving you a lot of time).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#for custom transformers three things needed\n",
    "#fit(), transform(), and fit_transform()\n",
    "#note you see 2 below, the third fit_transform()\n",
    "#is included by adding \"TransformerMixin\" as a base class\n",
    "#the \"BaseEstimator\" base class gives two free methods\n",
    "#get_params() and set_params() which are good for auto tuning\n",
    "#hyper params such a hyper param might be the stategy type\n",
    "#of median for the \"SimpleImputer\"\n",
    "from sklearn.base import BaseEstimator, TransformerMixin \n",
    "\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "#these are column indices for the dataset\n",
    "rooms_ix, bedrooms_ix, population_ix, households_ix = 3, 4, 5, 6 \n",
    "\n",
    "class CombinedAttributesAdder2(BaseEstimator, TransformerMixin): \n",
    "    \n",
    "    # no *args or ** kargs\n",
    "    #custom transformer to add the custom attribute \n",
    "    #bedrooms_per_room which combines total_bedrooms/total_rooms columns\n",
    "    def __init__(self, add_bedrooms_per_room = True): \n",
    "        #set boolean true?\n",
    "        self.add_bedrooms_per_room = add_bedrooms_per_room \n",
    "    def fit( self, X, y = None): \n",
    "            # nothing else to do\n",
    "            return self  \n",
    "    def transform(self, X): \n",
    "            \n",
    "            \n",
    "            #note here is what np.c_ does\n",
    "            #np.c_[np.array([1,2,3]), np.array([4,5,6])]\n",
    "            #array([[1, 4],\n",
    "            #[2, 5],\n",
    "            #[3, 6]])\n",
    "            #combines columns in input array into 2d array\n",
    "            if self.add_bedrooms_per_room: \n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                X = SelectKBest(chi2, k=2).fit_transform(housing_num, housing_labels)\n",
    "                print(X_new.info())\n",
    "                \n",
    "                \n",
    "                \n",
    "                return np.c_[X, rooms_per_household, population_per_household, bedrooms_per_room] \n",
    "                \n",
    "            else: \n",
    "                return np.c_[X, rooms_per_household, population_per_household] \n",
    "            \n",
    "            \n",
    "            attr_adder = CombinedAttributesAdder( add_bedrooms_per_room = False) \n",
    "            housing_extra_attribs = attr_adder.transform( housing.values)\n",
    "\n",
    "    \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.39096471861870913\n",
      "median_income\n",
      "0.1540596771986014\n",
      "INLAND\n",
      "0.07037646543846873\n",
      "rooms_per_household\n",
      "0.062222704591347\n",
      "longitude\n",
      "0.05761836862232709\n",
      "latitude\n",
      "0.05115095915174943\n",
      "pop_per_hhold\n",
      "0.04739145739413316\n",
      "population_per_household\n",
      "0.039102190630224826\n",
      "housing_median_age\n",
      "0.030523720849855766\n",
      "bedrooms_per_room\n",
      "0.021454506785598886\n",
      "bedrooms_per_room\n",
      "0.021311017913683856\n",
      "rooms_per_hhold\n",
      "0.012271076285395062\n",
      "total_rooms\n",
      "0.01185704786761798\n",
      "total_bedrooms\n",
      "0.011450822051574406\n",
      "population\n",
      "0.011148068290426286\n",
      "households\n",
      "0.003029048853168095\n",
      "<1H OCEAN\n",
      "0.002301684812100263\n",
      "NEAR OCEAN\n",
      "0.0017381923012956628\n",
      "NEAR BAY\n",
      "2.8272343722778223e-05\n",
      "ISLAND\n"
     ]
    }
   ],
   "source": [
    "feature_importances = search.best_estimator_.feature_importances_\n",
    "feature_importances\n",
    "extra_attribs = [\"rooms_per_hhold\", \"pop_per_hhold\", \"bedrooms_per_room\"]\n",
    "#cat_encoder = cat_pipeline.named_steps[\"cat_encoder\"] # old solution\n",
    "cat_encoder = full_pipeline.named_transformers_[\"cat\"]\n",
    "cat_one_hot_attribs = list(cat_encoder.categories_[0])\n",
    "attributes = num_attribs + extra_attribs + cat_one_hot_attribs\n",
    "mydata = sorted(zip(feature_importances, attributes), reverse=True)\n",
    "\n",
    "for attribute in mydata:\n",
    "    print(attribute[0])\n",
    "    print(attribute[1])\n",
    "    \n",
    "    if\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 16512 entries, 17606 to 15775\n",
      "Data columns (total 11 columns):\n",
      " #   Column                    Non-Null Count  Dtype  \n",
      "---  ------                    --------------  -----  \n",
      " 0   longitude                 16512 non-null  float64\n",
      " 1   latitude                  16512 non-null  float64\n",
      " 2   housing_median_age        16512 non-null  float64\n",
      " 3   total_rooms               16512 non-null  float64\n",
      " 4   total_bedrooms            16512 non-null  float64\n",
      " 5   population                16512 non-null  float64\n",
      " 6   households                16512 non-null  float64\n",
      " 7   median_income             16512 non-null  float64\n",
      " 8   rooms_per_household       16512 non-null  float64\n",
      " 9   bedrooms_per_room         16512 non-null  float64\n",
      " 10  population_per_household  16512 non-null  float64\n",
      "dtypes: float64(11)\n",
      "memory usage: 1.5 MB\n"
     ]
    }
   ],
   "source": [
    "housing_num.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# column index\n",
    "rooms_ix, bedrooms_ix, population_ix, households_ix = 3, 4, 5, 6\n",
    "\n",
    "class CombinedAttributesAdder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, add_bedrooms_per_room = True): # no *args or **kargs\n",
    "        self.add_bedrooms_per_room = add_bedrooms_per_room\n",
    "    def fit(self, X, y=None):\n",
    "        return self  # nothing else to do\n",
    "    def transform(self, X):\n",
    "        rooms_per_household = X[:, rooms_ix] / X[:, households_ix]\n",
    "        population_per_household = X[:, population_ix] / X[:, households_ix]\n",
    "        if self.add_bedrooms_per_room:\n",
    "            bedrooms_per_room = X[:, bedrooms_ix] / X[:, rooms_ix]\n",
    "            return np.c_[X, rooms_per_household, population_per_household,\n",
    "                         bedrooms_per_room]\n",
    "        else:\n",
    "            return np.c_[X, rooms_per_household, population_per_household]\n",
    "\n",
    "attr_adder = CombinedAttributesAdder(add_bedrooms_per_room=False)\n",
    "housing_extra_attribs = attr_adder.transform(housing.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1568.,  710.],\n",
       "       [ 679.,  306.],\n",
       "       [1952.,  936.],\n",
       "       ...,\n",
       "       [4855., 2098.],\n",
       "       [1960., 1356.],\n",
       "       [3095., 1269.]])"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "class FunctionFeaturizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, transformer):\n",
    "        self.transformer = transformer\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.transformer.fit(X, y)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        \n",
    "        X = self.transformer.transform(X)\n",
    "        \n",
    "        \n",
    "        return X\n",
    "ff = FunctionFeaturizer(SelectKBest(chi2, k=2))\n",
    "\n",
    "\n",
    "housing_no_negative = housing_num.drop(\"longitude\", axis = 1)\n",
    "housing_no_negative = housing_no_negative.drop(\"latitude\", axis = 1)\n",
    "ff.fit(housing_no_negative, housing_labels)\n",
    "ff.transform(housing_no_negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy=\"median\")),\n",
    "        ('attribs_adder', CombinedAttributesAdder()),\n",
    "        ('std_scaler', StandardScaler()),\n",
    "    ])\n",
    "\n",
    "housing_num_tr = num_pipeline.fit_transform(housing_num)\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "num_attribs = list(housing_num)\n",
    "cat_attribs = [\"ocean_proximity\"]\n",
    "\n",
    "full_pipeline = ColumnTransformer([\n",
    "        (\"num\", num_pipeline, num_attribs),\n",
    "        (\"cat\", OneHotEncoder(), cat_attribs),\n",
    "    ])\n",
    "\n",
    "housing_prepared = full_pipeline.fit_transform(housing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n",
      "19\n"
     ]
    }
   ],
   "source": [
    "print(len(housing_prepared[0]))\n",
    "extra_attribs = [\"rooms_per_hhold\", \"pop_per_hhold\", \"bedrooms_per_room\"]\n",
    "#cat_encoder = cat_pipeline.named_steps[\"cat_encoder\"] # old solution\n",
    "cat_encoder = full_pipeline.named_transformers_[\"cat\"]\n",
    "cat_one_hot_attribs = list(cat_encoder.categories_[0])\n",
    "attributes = num_attribs + extra_attribs + cat_one_hot_attribs\n",
    "print(len(attributes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 16512 entries, 17606 to 15775\n",
      "Data columns (total 11 columns):\n",
      " #   Column                    Non-Null Count  Dtype  \n",
      "---  ------                    --------------  -----  \n",
      " 0   longitude                 16512 non-null  float64\n",
      " 1   latitude                  16512 non-null  float64\n",
      " 2   housing_median_age        16512 non-null  float64\n",
      " 3   total_rooms               16512 non-null  float64\n",
      " 4   total_bedrooms            16512 non-null  float64\n",
      " 5   population                16512 non-null  float64\n",
      " 6   households                16512 non-null  float64\n",
      " 7   median_income             16512 non-null  float64\n",
      " 8   rooms_per_household       16512 non-null  float64\n",
      " 9   bedrooms_per_room         16512 non-null  float64\n",
      " 10  population_per_household  16512 non-null  float64\n",
      "dtypes: float64(11)\n",
      "memory usage: 1.5 MB\n"
     ]
    }
   ],
   "source": [
    "housing_num.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "median_income\n",
      "rooms_per_household\n",
      "[1, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]\n",
      "2\n",
      "final prediction is\n",
      "60563.333333333336\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def calculateprediction(val1, val2):\n",
    "\n",
    "    num_pipeline = Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy=\"median\")),\n",
    "            ('attribs_adder', CombinedAttributesAdder()),\n",
    "            ('std_scaler', StandardScaler()),\n",
    "        ])\n",
    "\n",
    "    housing_num_tr = num_pipeline.fit_transform(housing_num)\n",
    "\n",
    "    from sklearn.compose import ColumnTransformer\n",
    "\n",
    "    num_attribs = list(housing_num)\n",
    "    cat_attribs = [\"ocean_proximity\"]\n",
    "\n",
    "    full_pipeline = ColumnTransformer([\n",
    "            (\"num\", num_pipeline, num_attribs),\n",
    "            (\"cat\", OneHotEncoder(), cat_attribs),\n",
    "        ])\n",
    "\n",
    "    housing_prepared = full_pipeline.fit_transform(housing)\n",
    "\n",
    "\n",
    "    param_grid = [\n",
    "        # try 12 (3Ã—4) combinations of hyperparameters\n",
    "        {'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]},\n",
    "        # then try 6 (2Ã—3) combinations with bootstrap set as False\n",
    "        {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]},\n",
    "      ]\n",
    "\n",
    "    forest_reg = RandomForestRegressor(random_state=42)\n",
    "    # train across 5 folds, that's a total of (12+6)*5=90 rounds of training \n",
    "    grid_search = GridSearchCV(forest_reg, param_grid, cv=2,\n",
    "                               scoring='neg_mean_squared_error',\n",
    "                               return_train_score=True)\n",
    "    grid_search.fit(housing_prepared, housing_labels)\n",
    "\n",
    "\n",
    "    extra_attribs = [\"rooms_per_hhold\", \"pop_per_hhold\", \"bedrooms_per_room\"]\n",
    "    #cat_encoder = cat_pipeline.named_steps[\"cat_encoder\"] # old solution\n",
    "    cat_encoder = full_pipeline.named_transformers_[\"cat\"]\n",
    "    cat_one_hot_attribs = list(cat_encoder.categories_[0])\n",
    "    attributes = num_attribs + extra_attribs + cat_one_hot_attribs\n",
    "    mylist = sorted(zip(feature_importances, attributes), reverse=True)\n",
    "    index = 0\n",
    "    housing_prepared_cols_to_drop = []\n",
    "    for item in mylist:\n",
    "        if item[0] < 0.07 or (item[0] > 0.7 and item[0] < 0.2) or item[1] == \"INLAND\" :\n",
    "            housing_prepared_cols_to_drop.append(index)\n",
    "        else:\n",
    "            print(item[1])\n",
    "        index = index+1\n",
    "\n",
    "    \n",
    "\n",
    "    print(housing_prepared_cols_to_drop)    \n",
    "\n",
    "    housing_prepared = np.delete(housing_prepared, housing_prepared_cols_to_drop, axis=1)\n",
    "\n",
    "    print(len(housing_prepared[0]))\n",
    "    \n",
    "    param_grid = [\n",
    "        # try 12 (3Ã—4) combinations of hyperparameters\n",
    "        {'n_estimators': [3, 10, 30], 'max_features': [2]}\n",
    "      ]\n",
    "\n",
    "    forest_reg = RandomForestRegressor(random_state=42)\n",
    "  \n",
    "    grid_search = GridSearchCV(forest_reg, param_grid, cv=2,\n",
    "                               scoring='neg_mean_squared_error',\n",
    "                               return_train_score=True)\n",
    "    grid_search.fit(housing_prepared, housing_labels)\n",
    "    \n",
    "    final_model = grid_search.best_estimator_\n",
    "\n",
    "    mydata = np.array([[val1, val2]])\n",
    "\n",
    "    final_predictions = final_model.predict(mydata) \n",
    "    print(\"final prediction is\")\n",
    "    print(final_predictions[0])\n",
    "    \n",
    "calculateprediction(3, 4.5)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-244-78e28d44fe20>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[0mclf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRandomizedSearchCV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mforest_reg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdistributions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m \u001b[0msearch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhousing_prepared\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhousing_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m \u001b[0msearch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ml\\my_env\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     70\u001b[0m                           FutureWarning)\n\u001b[0;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ml\\my_env\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    734\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    735\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 736\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    737\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    738\u001b[0m         \u001b[1;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ml\\my_env\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1529\u001b[0m         evaluate_candidates(ParameterSampler(\n\u001b[0;32m   1530\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_distributions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1531\u001b[1;33m             random_state=self.random_state))\n\u001b[0m",
      "\u001b[1;32mc:\\ml\\my_env\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[1;34m(candidate_params)\u001b[0m\n\u001b[0;32m    713\u001b[0m                                \u001b[1;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    714\u001b[0m                                in product(candidate_params,\n\u001b[1;32m--> 715\u001b[1;33m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[0;32m    716\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    717\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ml\\my_env\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1030\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1031\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1032\u001b[1;33m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1033\u001b[0m                 \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1034\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ml\\my_env\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    845\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    846\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 847\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    848\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ml\\my_env\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    763\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    764\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 765\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    766\u001b[0m             \u001b[1;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    767\u001b[0m             \u001b[1;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ml\\my_env\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    209\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ml\\my_env\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    570\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    571\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 572\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ml\\my_env\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    251\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    252\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 253\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    254\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ml\\my_env\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    251\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    252\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 253\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    254\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ml\\my_env\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, error_score)\u001b[0m\n\u001b[0;32m    529\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    530\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 531\u001b[1;33m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    532\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    533\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ml\\my_env\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    390\u001b[0m                     \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    391\u001b[0m                     n_samples_bootstrap=n_samples_bootstrap)\n\u001b[1;32m--> 392\u001b[1;33m                 for i, t in enumerate(trees))\n\u001b[0m\u001b[0;32m    393\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    394\u001b[0m             \u001b[1;31m# Collect newly grown trees\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ml\\my_env\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1030\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1031\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1032\u001b[1;33m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1033\u001b[0m                 \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1034\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ml\\my_env\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    845\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    846\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 847\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    848\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ml\\my_env\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    763\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    764\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 765\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    766\u001b[0m             \u001b[1;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    767\u001b[0m             \u001b[1;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ml\\my_env\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    209\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ml\\my_env\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    570\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    571\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 572\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ml\\my_env\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    251\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    252\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 253\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    254\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ml\\my_env\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    251\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    252\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 253\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    254\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ml\\my_env\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\u001b[0m in \u001b[0;36m_parallel_build_trees\u001b[1;34m(tree, forest, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[0;32m    148\u001b[0m         \u001b[0mn_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m             \u001b[0mcurr_sample_weight\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m             \u001b[0mcurr_sample_weight\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ml\\my_env\\lib\\site-packages\\numpy\\core\\numeric.py\u001b[0m in \u001b[0;36mones\u001b[1;34m(shape, dtype, order)\u001b[0m\n\u001b[0;32m    191\u001b[0m     \"\"\"\n\u001b[0;32m    192\u001b[0m     \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 193\u001b[1;33m     \u001b[0mmultiarray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopyto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'unsafe'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    194\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mcopyto\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#fine tune the hyperparameters of random forest using GridSearchCV\n",
    "from sklearn.model_selection import GridSearchCV \n",
    "\n",
    "#try using 3, 10, and 30 estimators, 2, 4, 6, and 8 max features\n",
    "#false the bootstrap(some resampling technique)\n",
    "#note we have two arrays in here each with their tweaks\n",
    "# param_grid = [ {'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]}, \n",
    "#               {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]}, ]\n",
    "\n",
    "forest_reg = RandomForestRegressor() \n",
    "\n",
    "#note cv = 5 means five fold cross validation\n",
    "#previously we were using 10 fold\n",
    "# grid_search = GridSearchCV( forest_reg, param_grid, cv = 5, \n",
    "#                            scoring ='neg_mean_squared_error', \n",
    "#                            return_train_score = True) \n",
    "# grid_search.fit( housing_prepared, housing_labels)\n",
    "    \n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "distributions = dict({'n_estimators': [3, 10, 15, 17, 18, 30, 31, 32], 'max_features': [1, 2, 3]}, \n",
    "               )\n",
    "\n",
    "\n",
    "clf = RandomizedSearchCV(forest_reg, distributions, random_state=0)\n",
    "search = clf.fit(housing_prepared, housing_labels)\n",
    "search.best_params_\n",
    "\n",
    "final_model = search.best_estimator_\n",
    "X_test = strat_test_set.drop(\"median_house_value\", axis = 1)\n",
    "y_test = strat_test_set[\"median_house_value\"].copy() \n",
    "X_test_prepared = full_pipeline.transform( X_test) \n",
    "final_predictions = final_model.predict(X_test_prepared) \n",
    "final_mse = mean_squared_error( y_test, final_predictions) \n",
    "final_rmse = np.sqrt(final_mse)\n",
    "print(final_rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 17, 'max_features': 2}\n",
      "51106.46635382059\n",
      "{'n_estimators': 15, 'max_features': 2}\n",
      "51562.23725262591\n",
      "{'n_estimators': 31, 'max_features': 3}\n",
      "48878.09064216018\n",
      "{'n_estimators': 31, 'max_features': 2}\n",
      "49834.09450731377\n",
      "{'n_estimators': 18, 'max_features': 3}\n",
      "50004.86507383377\n",
      "{'n_estimators': 10, 'max_features': 1}\n",
      "56922.710364170816\n",
      "{'n_estimators': 30, 'max_features': 2}\n",
      "50524.78217689312\n",
      "{'n_estimators': 32, 'max_features': 3}\n",
      "48789.03004215762\n",
      "{'n_estimators': 3, 'max_features': 3}\n",
      "60056.905175994274\n",
      "{'n_estimators': 3, 'max_features': 2}\n",
      "61140.45284006917\n"
     ]
    }
   ],
   "source": [
    "#testing randoms best RMSE \n",
    "#not so hot...\n",
    "\n",
    "#able to print the results myself\n",
    "cvres = search.cv_results_\n",
    "for mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n",
    "    \n",
    "    print(params)\n",
    "    num_estim = params['n_estimators']\n",
    "    max_feat = params['max_features']\n",
    "    if(max_feat < 8):\n",
    "        final_model = RandomForestRegressor(n_estimators=num_estim, max_features=max_feat).fit(housing_prepared, housing_labels)\n",
    "        X_test = strat_test_set.drop(\"median_house_value\", axis = 1) \n",
    "        y_test = strat_test_set[\"median_house_value\"].copy() \n",
    "        X_test_prepared = full_pipeline.transform( X_test) \n",
    "        final_predictions = final_model.predict(X_test_prepared) \n",
    "        final_mse = mean_squared_error( y_test, final_predictions) \n",
    "        final_rmse = np.sqrt(final_mse)\n",
    "        print(final_rmse)\n",
    "\n",
    "# for i in search.cv_results_:\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "#tip\n",
    "# When you have no idea what value a hyperparameter should have, a simple\n",
    "# approach is to try out consecutive powers of 10 (or a smaller number if\n",
    "# you want a more fine-grained search, as shown in this example with the n_estimators\n",
    "# hyperparameter).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=RandomForestRegressor(random_state=42),\n",
       "             param_grid=[{'max_features': [2, 4, 6, 8],\n",
       "                          'n_estimators': [3, 10, 30]},\n",
       "                         {'bootstrap': [False], 'max_features': [2, 3, 4],\n",
       "                          'n_estimators': [3, 10]}],\n",
       "             refit=False, return_train_score=True,\n",
       "             scoring='neg_mean_squared_error')"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = [\n",
    "    # try 12 (3Ã—4) combinations of hyperparameters\n",
    "    {'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]},\n",
    "    # then try 6 (2Ã—3) combinations with bootstrap set as False\n",
    "    {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]},\n",
    "  ]\n",
    "\n",
    "forest_reg = RandomForestRegressor(random_state=42)\n",
    "# train across 5 folds, that's a total of (12+6)*5=90 rounds of training \n",
    "grid_search = GridSearchCV(forest_reg, param_grid, cv=5,\n",
    "                           scoring='neg_mean_squared_error',\n",
    "                           return_train_score=True, refit=False)\n",
    "grid_search.fit(housing_prepared, housing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63887.20816960544 {'max_features': 2, 'n_estimators': 3}\n",
      "55193.750552471996 {'max_features': 2, 'n_estimators': 10}\n",
      "52811.39751490818 {'max_features': 2, 'n_estimators': 30}\n",
      "60484.54870180369 {'max_features': 4, 'n_estimators': 3}\n",
      "52343.04972161877 {'max_features': 4, 'n_estimators': 10}\n",
      "50230.90257901035 {'max_features': 4, 'n_estimators': 30}\n",
      "57940.63230331034 {'max_features': 6, 'n_estimators': 3}\n",
      "51550.24225112176 {'max_features': 6, 'n_estimators': 10}\n",
      "49477.6717453135 {'max_features': 6, 'n_estimators': 30}\n",
      "57630.706930867615 {'max_features': 8, 'n_estimators': 3}\n",
      "51310.953199060605 {'max_features': 8, 'n_estimators': 10}\n",
      "49525.76768441087 {'max_features': 8, 'n_estimators': 30}\n",
      "61932.69251042756 {'bootstrap': False, 'max_features': 2, 'n_estimators': 3}\n",
      "53897.28492437724 {'bootstrap': False, 'max_features': 2, 'n_estimators': 10}\n",
      "57637.812939854295 {'bootstrap': False, 'max_features': 3, 'n_estimators': 3}\n",
      "52114.924626877764 {'bootstrap': False, 'max_features': 3, 'n_estimators': 10}\n",
      "57469.95679340483 {'bootstrap': False, 'max_features': 4, 'n_estimators': 3}\n",
      "51173.15342627264 {'bootstrap': False, 'max_features': 4, 'n_estimators': 10}\n"
     ]
    }
   ],
   "source": [
    "cvres = grid_search.cv_results_\n",
    "for mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n",
    "    print(np.sqrt(-mean_score), params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline \n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# selector = SelectFromModel(estimator=LogisticRegression()).fit(housing.iloc[:100], housing_labels.iloc[:100])\n",
    "\n",
    "\n",
    "\n",
    "#set the sequence of transformations\n",
    "num_pipeline = Pipeline([ ('imputer', SimpleImputer( strategy =\"median\")), \n",
    "                         ('attribs_adder', CombinedAttributesAdder()), \n",
    "                         ('selector', SelectFromModel(estimator=LogisticRegression())),\n",
    "                         ('std_scaler', StandardScaler()),]) \n",
    "#apply the sequence\n",
    "housing_num_tr = num_pipeline.fit_transform(housing_num)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#this cell takes the \"ocean_proximity\" column and transforms it\n",
    "#to be numerical like the rest of our columns,\n",
    "#it does so using pipeline\n",
    "#we use \"ColumnTransformer\" as well\n",
    "from sklearn.compose import ColumnTransformer \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#get numerical columns\n",
    "num_attribs = list( housing_num) \n",
    "#get text columns\n",
    "cat_attribs = [\"ocean_proximity\"] \n",
    "\n",
    "\n",
    "\n",
    "#apply numerical pipelines to num columns portion of data\n",
    "#apply one hot to category column\n",
    "full_pipeline = ColumnTransformer([ (\"num\", num_pipeline, num_attribs), \n",
    "                                   (\"cat\", OneHotEncoder(), cat_attribs),]) \n",
    "\n",
    "#apply the full pipeline to all of the data\n",
    "housing_prepared = full_pipeline.fit_transform( housing)\n",
    "\n",
    "#fine tune the hyperparameters of random forest using GridSearchCV\n",
    "from sklearn.model_selection import GridSearchCV \n",
    "\n",
    "#try using 3, 10, and 30 estimators, 2, 4, 6, and 8 max features\n",
    "#false the bootstrap(some resampling technique)\n",
    "#note we have two arrays in here each with their tweaks\n",
    "# param_grid = [ {'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]}, \n",
    "#               {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]}, ]\n",
    "\n",
    "forest_reg = RandomForestRegressor() \n",
    "\n",
    "#note cv = 5 means five fold cross validation\n",
    "#previously we were using 10 fold\n",
    "# grid_search = GridSearchCV( forest_reg, param_grid, cv = 5, \n",
    "#                            scoring ='neg_mean_squared_error', \n",
    "#                            return_train_score = True) \n",
    "# grid_search.fit( housing_prepared, housing_labels)\n",
    "    \n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "distributions = dict({'n_estimators': [3, 10, 15, 17, 18, 30, 31, 32], 'max_features': [2, 4, 6, 8, 10, 12, 14]}, \n",
    "               )\n",
    "\n",
    "\n",
    "clf = RandomizedSearchCV(forest_reg, distributions, random_state=0)\n",
    "search = clf.fit(housing_prepared, housing_labels)\n",
    "print(\"best params\")\n",
    "search.best_params_\n",
    "\n",
    "cvres = search.cv_results_\n",
    "for mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n",
    "    print(params)\n",
    "    num_estim = params['n_estimators']\n",
    "    max_feat = params['max_features']\n",
    "    final_model = RandomForestRegressor(n_estimators=num_estim, max_features=max_feat).fit(housing_prepared, housing_labels)\n",
    "    X_test = strat_test_set.drop(\"median_house_value\", axis = 1) \n",
    "    y_test = strat_test_set[\"median_house_value\"].copy() \n",
    "    X_test_prepared = full_pipeline.transform( X_test) \n",
    "    final_predictions = final_model.predict(X_test_prepared) \n",
    "    final_mse = mean_squared_error( y_test, final_predictions) \n",
    "    final_rmse = np.sqrt(final_mse)\n",
    "    print(final_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-135-c7a145588daa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m num_pipeline = Pipeline([ \n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m     \u001b[1;33m(\u001b[0m\u001b[1;34m'feature_selection'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSelectFromModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLogisticRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhousing_num\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhousing_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m                             ('classification', RandomForestClassifier())]) \n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ml\\my_env\\lib\\site-packages\\sklearn\\feature_selection\\_from_model.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    222\u001b[0m                 \"Since 'prefit=True', call transform directly\")\n\u001b[0;32m    223\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimator_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 224\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimator_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    225\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    226\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ml\\my_env\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1342\u001b[0m         X, y = self._validate_data(X, y, accept_sparse='csr', dtype=_dtype,\n\u001b[0;32m   1343\u001b[0m                                    \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"C\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1344\u001b[1;33m                                    accept_large_sparse=solver != 'liblinear')\n\u001b[0m\u001b[0;32m   1345\u001b[0m         \u001b[0mcheck_classification_targets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1346\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ml\\my_env\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    430\u001b[0m                 \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    431\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 432\u001b[1;33m                 \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    433\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    434\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ml\\my_env\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     70\u001b[0m                           FutureWarning)\n\u001b[0;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ml\\my_env\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m    800\u001b[0m                     \u001b[0mensure_min_samples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mensure_min_samples\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    801\u001b[0m                     \u001b[0mensure_min_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mensure_min_features\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 802\u001b[1;33m                     estimator=estimator)\n\u001b[0m\u001b[0;32m    803\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    804\u001b[0m         y = check_array(y, accept_sparse='csr', force_all_finite=True,\n",
      "\u001b[1;32mc:\\ml\\my_env\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     70\u001b[0m                           FutureWarning)\n\u001b[0;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ml\\my_env\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    643\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    644\u001b[0m             _assert_all_finite(array,\n\u001b[1;32m--> 645\u001b[1;33m                                allow_nan=force_all_finite == 'allow-nan')\n\u001b[0m\u001b[0;32m    646\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    647\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mensure_min_samples\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ml\\my_env\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype)\u001b[0m\n\u001b[0;32m     97\u001b[0m                     \u001b[0mmsg_err\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m                     (type_err,\n\u001b[1;32m---> 99\u001b[1;33m                      msg_dtype if msg_dtype is not None else X.dtype)\n\u001b[0m\u001b[0;32m    100\u001b[0m             )\n\u001b[0;32m    101\u001b[0m     \u001b[1;31m# for object dtype data, we only check for NaNs (GH-13254)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline \n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# selector = SelectFromModel(estimator=LogisticRegression()).fit(housing.iloc[:100], housing_labels.iloc[:100])\n",
    "\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "def all_but_last_column(X):\n",
    "    return X[:,:-1]\n",
    "\n",
    "#set the sequence of transformations\n",
    "num_pipeline = Pipeline([ \n",
    "                         \n",
    "    ('feature_selection', SelectFromModel(LogisticRegression()).fit(housing_num, housing_labels)),\n",
    "                            ('classification', RandomForestClassifier())]) \n",
    "\n",
    "\n",
    "#apply the sequence\n",
    "housing_num_tr = num_pipeline.fit_transform(housing_num)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#this cell takes the \"ocean_proximity\" column and transforms it\n",
    "#to be numerical like the rest of our columns,\n",
    "#it does so using pipeline\n",
    "#we use \"ColumnTransformer\" as well\n",
    "from sklearn.compose import ColumnTransformer \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#get numerical columns\n",
    "num_attribs = list( housing_num) \n",
    "#get text columns\n",
    "cat_attribs = [\"ocean_proximity\"] \n",
    "\n",
    "\n",
    "\n",
    "#apply numerical pipelines to num columns portion of data\n",
    "#apply one hot to category column\n",
    "full_pipeline = ColumnTransformer([ (\"num\", num_pipeline, num_attribs), \n",
    "                                   (\"cat\", OneHotEncoder(), cat_attribs),]) \n",
    "\n",
    "#apply the full pipeline to all of the data\n",
    "housing_prepared = full_pipeline.fit_transform( housing)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to get best params found by GridSearchCV \n",
    "grid_search.best_params_\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this could just be the highest values...\n",
    "#perhaps set higher and see if there's a better middle ground?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fine tune the hyperparameters of random forest using GridSearchCV\n",
    "from sklearn.model_selection import GridSearchCV \n",
    "\n",
    "#try using 3, 10, and 30 estimators, 2, 4, 6, and 8 max features\n",
    "#false the bootstrap(some resampling technique)\n",
    "#note we have two arrays in here each with their tweaks\n",
    "param_grid = [ {'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8, 10, 12, 14, 16, 18]}, \n",
    "              {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]}, ]\n",
    "\n",
    "forest_reg = RandomForestRegressor() \n",
    "\n",
    "#note cv = 5 means five fold cross validation\n",
    "#previously we were using 10 fold\n",
    "grid_search = GridSearchCV( forest_reg, param_grid, cv = 5, \n",
    "                           scoring ='neg_mean_squared_error', \n",
    "                           return_train_score = True) \n",
    "grid_search.fit( housing_prepared, housing_labels)\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note \n",
    "# If GridSearchCV is initialized with refit = True (which is the default), then once\n",
    "# it finds the best estimator using cross-validation, it retrains it on the whole training set.\n",
    "# This is usually a good idea, since feeding it more data will likely improve its performance.\n",
    "\n",
    "#^ good ovbiously this lets us improve how the algorithm generalizes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let us see if upping the number of max features in GridSearchCV had any effect..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new best params\n",
    "grid_search.best_params_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#interesting.... now max_features 6 is better despite that already existing... no matter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loop through all possibilities tested by GridSearchCV and see their cross validation scores\n",
    "cvres = grid_search.cv_results_ \n",
    "for mean_score, params in zip( cvres[\"mean_test_score\"], cvres[\"params\"]): \n",
    "    print( np.sqrt(-mean_score), params)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#indeed this was best... 49797.364928280964 {'max_features': 6, 'n_estimators': 30}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fine tune the hyperparameters of random forest using GridSearchCV\n",
    "from sklearn.model_selection import GridSearchCV \n",
    "\n",
    "#try using 3, 10, and 30 estimators, 2, 4, 6, and 8 max features\n",
    "#false the bootstrap(some resampling technique)\n",
    "#note we have two arrays in here each with their tweaks\n",
    "param_grid = [ {'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]}, \n",
    "              {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]}, ]\n",
    "\n",
    "forest_reg = RandomForestRegressor() \n",
    "\n",
    "#note cv = 5 means five fold cross validation\n",
    "#previously we were using 10 fold\n",
    "grid_search = GridSearchCV( forest_reg, param_grid, cv = 5, \n",
    "                           scoring ='neg_mean_squared_error', \n",
    "                           return_train_score = True) \n",
    "grid_search.fit( housing_prepared, housing_labels)\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_params_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#interesting... now it gets 6 and 30 again.. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this fine tunes the forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = grid_search.best_estimator_.feature_importances_ \n",
    "feature_importances\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this tells us the importance of each attribute (EG latitude, bedrooms, etc) in making accurate predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_attribs = [\"rooms_per_hhold\", \"pop_per_hhold\", \"bedrooms_per_room\"]\n",
    "cat_encoder = full_pipeline.named_transformers_[\"cat\"] \n",
    "cat_one_hot_attribs = list(cat_encoder.categories_[ 0])\n",
    "attributes = num_attribs + extra_attribs + cat_one_hot_attribs \n",
    "sorted( zip( feature_importances, attributes), reverse = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#island is obviously an outlier so obviously it has very little effect\n",
    "#I would argue it could be removed all togehter..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#notice Inland is the only useful looking property...\n",
    "#could perhaps create a column that says "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check our test set finally! \n",
    "\n",
    "final_model = grid_search.best_estimator_ \n",
    "#inputs, do not include the house value as that is what we want to predict\n",
    "X_test = strat_test_set.drop(\"median_house_value\", axis = 1) \n",
    "#outputs to compare against\n",
    "y_test = strat_test_set[\"median_house_value\"].copy() \n",
    "X_test_prepared = full_pipeline.transform(X_test) \n",
    "final_predictions = final_model.predict(X_test_prepared) \n",
    "final_mse = mean_squared_error( y_test, final_predictions) \n",
    "# = > evaluates to 47,730.2\n",
    "final_rmse = np.sqrt(final_mse) \n",
    "\n",
    "final_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I got a better score than the book :D (thank you GridSearchCV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "confidence = 0.95\n",
    "squared_errors = (final_predictions - y_test) ** 2 \n",
    "#stats.t.interval says: \"Endpoints of the range that contains alpha percent of the distribution\"\n",
    "#alpha being 0.95 for us\n",
    "#loc shifts the the distribution by the average squared errors\n",
    "#the scale scales the distribution by the standarm error of the mean of the squared errors\n",
    "#so 95% of the distribution lies between 45696.58 and 49535.75?\n",
    "#is this saying that's where the error lies? or that 95% of the data points\n",
    "#that contributed to the RMSE lie there?\n",
    "np.sqrt(stats.t.interval( confidence, len( squared_errors) - 1, \n",
    "                          loc = squared_errors.mean(),\n",
    "                          scale = stats.sem( squared_errors)))\n",
    "\n",
    "\n",
    "#the confidence interval tells us that indeed our true \n",
    "#amount of RMSE lies somewhere between 45695 and 49535\n",
    "#so in this case we are on the right track\n",
    "\n",
    "#however as mentioned below if we see a drop here we must not tweak the hyperparameters\n",
    "#to perform better on the test set as it would likely not generalize all that well \n",
    "#to new data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you did a lot of hyperparameter tuning, the performance will usually be slightly worse than what you \n",
    "# measured using cross-validation (because your system ends up fine-tuned to perform well on the \n",
    "# validation data and will likely not perform as well on unknown datasets).\n",
    "# It is not the case in this example, but when this happens you must resist \n",
    "# the temptation to tweak the hyperparameters to make the numbers look good on the\n",
    "# test set; the improvements would be unlikely to generalize to new data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PreLaunch Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mean_absolute_percentage_error(y_test, final_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DONE with chapter 2 good job! Now to test some of the exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing SVM with kernel rbf\n",
    "from sklearn import svm\n",
    "\n",
    "\n",
    "#set the kernel and C, gamma, and epsilon values\n",
    "svm_reg = svm.SVR(kernel='rbf', C=100, gamma=0.1, epsilon=.1)\n",
    "svm_reg.fit(housing_prepared, housing_labels)\n",
    "\n",
    "\n",
    "housing_predictions = svm_reg.predict(housing_prepared) \n",
    "svm_mse = mean_squared_error(housing_labels, housing_predictions) \n",
    "svm_rmse = np.sqrt(svm_mse) \n",
    "\n",
    "svm_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_scores = cross_val_score(svm_reg, housing_prepared, housing_labels, \n",
    "                             scoring =\"neg_mean_squared_error\", \n",
    "                             cv = 10) \n",
    "svm_rmse_scores = np.sqrt(-svm_scores) \n",
    "display_scores(svm_rmse_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RandomForestRegressor().get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exercise 2 testing out randomized search \n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "\n",
    "\n",
    "forest_reg = RandomForestRegressor() \n",
    "\n",
    "# specify parameters and distributions to sample from\n",
    "param_dist = {'n_estimators': [3, 10], \n",
    "              'max_features': [2, 4, 6, 8]}\n",
    "\n",
    "# run randomized search\n",
    "n_iter_search = 100\n",
    "random_search = RandomizedSearchCV(forest_reg, param_distributions=param_dist,\n",
    "                                   n_iter=n_iter_search)\n",
    "\n",
    "\n",
    "random_search.fit(housing_prepared, housing_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exercise 2 testing out randomized search \n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "\n",
    "\n",
    "forest_reg = RandomForestRegressor() \n",
    "\n",
    "# specify parameters and distributions to sample from\n",
    "param_dist = {'n_estimators': [], \n",
    "              'max_features': []}\n",
    "\n",
    "# run randomized search\n",
    "n_iter_search = 100\n",
    "random_search = RandomizedSearchCV(forest_reg, param_distributions=param_dist,\n",
    "                                   n_iter=n_iter_search)\n",
    "\n",
    "\n",
    "random_search.fit(housing_prepared, housing_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exercise 2 testing out randomized search \n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "\n",
    "\n",
    "forest_reg = RandomForestRegressor() \n",
    "\n",
    "# specify parameters and distributions to sample from\n",
    "param_dist = {'n_estimators': [3, 10], \n",
    "              'max_features': [2, 4, 6, 8]}\n",
    "\n",
    "# run randomized search\n",
    "n_iter_search = 100\n",
    "random_search = RandomizedSearchCV(forest_reg, param_distributions=param_dist,\n",
    "                                   n_iter=n_iter_search)\n",
    "\n",
    "\n",
    "random_search.fit(housing_prepared, housing_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvres = random_search.cv_results_ \n",
    "for mean_score, params in zip( cvres[\"mean_test_score\"], cvres[\"params\"]): \n",
    "    print( np.sqrt(-mean_score), params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report(results, n_top=3):\n",
    "    for i in range(1, n_top + 1):\n",
    "        candidates = np.flatnonzero(results['rank_test_score'] == i)\n",
    "        for candidate in candidates:\n",
    "            print(\"Model with rank: {0}\".format(i))\n",
    "            print(\"Mean validation score: {0:.3f} (std: {1:.3f})\"\n",
    "                  .format(results['mean_test_score'][candidate],\n",
    "                          results['std_test_score'][candidate]))\n",
    "            print(\"Parameters: {0}\".format(results['params'][candidate]))\n",
    "            print(\"\")\n",
    "            \n",
    "            \n",
    "report(random_search.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvres = random_search.cv_results_ \n",
    "for mean_score, params in zip( cvres[\"mean_test_score\"], cvres[\"params\"]): \n",
    "    print( np.sqrt(-mean_score), params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exercise 2 testing out randomized search \n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "\n",
    "\n",
    "forest_reg = RandomForestRegressor() \n",
    "\n",
    "# specify parameters and distributions to sample from\n",
    "param_dist = {'n_estimators': (0, 100), \n",
    "              'max_features': (0, 100)}\n",
    "\n",
    "# run randomized search\n",
    "n_iter_search = 100\n",
    "random_search = RandomizedSearchCV(forest_reg, param_distributions=param_dist,\n",
    "                                   n_iter=n_iter_search)\n",
    "\n",
    "\n",
    "random_search.fit(housing_prepared, housing_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exercise 2 testing out randomized search \n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "\n",
    "\n",
    "forest_reg = RandomForestRegressor() \n",
    "\n",
    "# specify parameters and distributions to sample from\n",
    "param_dist = {'n_estimators': (0, 100), \n",
    "              'max_features': (0, 100)}\n",
    "\n",
    "# run randomized search\n",
    "n_iter_search = 100\n",
    "random_search = RandomizedSearchCV(forest_reg, param_distributions=param_dist,\n",
    "                                   n_iter=n_iter_search)\n",
    "\n",
    "\n",
    "random_search.fit(housing_prepared, housing_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exercise 2 testing out randomized search \n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "\n",
    "\n",
    "forest_reg = RandomForestRegressor() \n",
    "\n",
    "# specify parameters and distributions to sample from\n",
    "param_dist = {'n_estimators': [2, 4, 6 , 8, 10, 12, 14, 16, 18], \n",
    "              'max_features': [2, 3, 4, 5, 6]}\n",
    "\n",
    "# run randomized search\n",
    "n_iter_search = 10\n",
    "random_search = RandomizedSearchCV(forest_reg, param_distributions=param_dist,\n",
    "                                   n_iter=n_iter_search)\n",
    "\n",
    "\n",
    "random_search.fit(housing_prepared, housing_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvres2 = random_search.cv_results_ \n",
    "for mean_score, params in zip( cvres2[\"mean_test_score\"], cvres[\"params\"]): \n",
    "    print( np.sqrt(-mean_score), params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvres2 = random_search.cv_results_ \n",
    "\n",
    "print(cvres2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvres = random_search.cv_results_ \n",
    "for mean_score, params in zip(cvres['params']): \n",
    "    print( params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvres = grid_search.cv_results_ \n",
    "for mean_score, params in zip( cvres[\"mean_test_score\"], cvres['params']): \n",
    "    print( np.sqrt(-mean_score), params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvres = grid_search.cv_results_ \n",
    "print(cvres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvres = random_search.cv_results_ \n",
    "for mean_score, params in zip(cvres['params']): \n",
    "    print( params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline \n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# selector = SelectFromModel(estimator=LogisticRegression()).fit(housing.iloc[:100], housing_labels.iloc[:100])\n",
    "\n",
    "\n",
    "\n",
    "#set the sequence of transformations\n",
    "num_pipeline = Pipeline([ ('imputer', SimpleImputer( strategy =\"median\")), \n",
    "                         ('attribs_adder', CombinedAttributesAdder()), \n",
    "                         ('selector', SelectFromModel(estimator=LogisticRegression())),\n",
    "                         ('std_scaler', StandardScaler()),]) \n",
    "#apply the sequence\n",
    "housing_num_tr = num_pipeline.fit_transform(housing_num)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#this cell takes the \"ocean_proximity\" column and transforms it\n",
    "#to be numerical like the rest of our columns,\n",
    "#it does so using pipeline\n",
    "#we use \"ColumnTransformer\" as well\n",
    "from sklearn.compose import ColumnTransformer \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#get numerical columns\n",
    "num_attribs = list( housing_num) \n",
    "#get text columns\n",
    "cat_attribs = [\"ocean_proximity\"] \n",
    "\n",
    "\n",
    "\n",
    "#apply numerical pipelines to num columns portion of data\n",
    "#apply one hot to category column\n",
    "full_pipeline = ColumnTransformer([ (\"num\", num_pipeline, num_attribs), \n",
    "                                   (\"cat\", OneHotEncoder(), cat_attribs),]) \n",
    "\n",
    "#apply the full pipeline to all of the data\n",
    "housing_prepared = full_pipeline.fit_transform( housing)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class RemoveThreeColumns(BaseEstimator, TransformerMixin): \n",
    "    \n",
    "    # no *args or ** kargs\n",
    "    #custom transformer to add the custom attribute \n",
    "    #bedrooms_per_room which combines total_bedrooms/total_rooms columns\n",
    "    def __init__(self, remove_columns = True): \n",
    "        #set boolean true?\n",
    "        self.remove_columns = remove_columns\n",
    "    def fit(self, X, y = None): \n",
    "            # nothing else to do\n",
    "            return self  \n",
    "    def transform(self, X): \n",
    "            \n",
    "            X_ = X.copy()\n",
    "            \n",
    "            X_ = X_[:,:1]\n",
    "            X_ = X_[:,:2]\n",
    "            X_ = X_[:,:3]\n",
    "            \n",
    "            print(\"new X_\")\n",
    "            print(X_)\n",
    "            \n",
    "            return X_\n",
    "        \n",
    "            #attr_adder = CombinedAttributesAdder( add_bedrooms_per_room = False) \n",
    "            #housing_extra_attribs = attr_adder.transform( housing.values)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline \n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# selector = SelectFromModel(estimator=LogisticRegression()).fit(housing.iloc[:100], housing_labels.iloc[:100])\n",
    "\n",
    "\n",
    "\n",
    "#set the sequence of transformations\n",
    "num_pipeline = Pipeline([ ('imputer', SimpleImputer( strategy =\"median\")), \n",
    "                         ('attribs_adder', CombinedAttributesAdder()), \n",
    "                         ('std_scaler', StandardScaler()),]) \n",
    "#apply the sequence\n",
    "housing_num_tr = num_pipeline.fit_transform(housing_num)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#this cell takes the \"ocean_proximity\" column and transforms it\n",
    "#to be numerical like the rest of our columns,\n",
    "#it does so using pipeline\n",
    "#we use \"ColumnTransformer\" as well\n",
    "from sklearn.compose import ColumnTransformer \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#get numerical columns\n",
    "num_attribs = list( housing_num) \n",
    "#get text columns\n",
    "cat_attribs = [\"ocean_proximity\"] \n",
    "\n",
    "\n",
    "\n",
    "#apply numerical pipelines to num columns portion of data\n",
    "#apply one hot to category column\n",
    "full_pipeline = ColumnTransformer([ (\"num\", num_pipeline, num_attribs), \n",
    "                                   (\"cat\", OneHotEncoder(), cat_attribs),]) \n",
    "\n",
    "#apply the full pipeline to all of the data\n",
    "housing_prepared = full_pipeline.fit_transform( housing)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor \n",
    "\n",
    "forest_reg = RandomForestRegressor() \n",
    "forest_reg.fit( housing_prepared, housing_labels)\n",
    "\n",
    "housing_predictions = forest_reg.predict(housing_prepared) \n",
    "forest_mse = mean_squared_error(housing_labels, housing_predictions) \n",
    "forest_rmse = np.sqrt(forest_mse) \n",
    "\n",
    "forest_rmse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_attribs = [\"rooms_per_hhold\", \"pop_per_hhold\", \"bedrooms_per_room\"]\n",
    "cat_encoder = full_pipeline.named_transformers_[\"cat\"] \n",
    "cat_one_hot_attribs = list(cat_encoder.categories_[ 0])\n",
    "attributes = num_attribs + extra_attribs + cat_one_hot_attribs \n",
    "sorted( zip( feature_importances, attributes), reverse = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class RemoveThreeColumns(BaseEstimator, TransformerMixin): \n",
    "    \n",
    "    # no *args or ** kargs\n",
    "    #custom transformer to add the custom attribute \n",
    "    #bedrooms_per_room which combines total_bedrooms/total_rooms columns\n",
    "    def __init__(self, remove_columns = True): \n",
    "        #set boolean true?\n",
    "        self.remove_columns = remove_columns\n",
    "    def fit(self, X, y = None): \n",
    "            # nothing else to do\n",
    "            return self  \n",
    "    def transform(self, X): \n",
    "            \n",
    "            X_ = X.copy()\n",
    "            \n",
    "            X_ = X_[:,0]\n",
    "            \n",
    "            print(\"new X_\")\n",
    "            print(X_)\n",
    "            \n",
    "            return X_\n",
    "        \n",
    "            #attr_adder = CombinedAttributesAdder( add_bedrooms_per_room = False) \n",
    "            #housing_extra_attribs = attr_adder.transform( housing.values)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline \n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# selector = SelectFromModel(estimator=LogisticRegression()).fit(housing.iloc[:100], housing_labels.iloc[:100])\n",
    "\n",
    "\n",
    "\n",
    "#set the sequence of transformations\n",
    "num_pipeline = Pipeline([ ('imputer', SimpleImputer( strategy =\"median\")), \n",
    "                         ('attribs_adder', CombinedAttributesAdder()), \n",
    "                         ('attribs_remover', RemoveThreeColumns()),\n",
    "                         ('std_scaler', StandardScaler()),]) \n",
    "#apply the sequence\n",
    "housing_num_tr = num_pipeline.fit_transform(housing_num)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#this cell takes the \"ocean_proximity\" column and transforms it\n",
    "#to be numerical like the rest of our columns,\n",
    "#it does so using pipeline\n",
    "#we use \"ColumnTransformer\" as well\n",
    "from sklearn.compose import ColumnTransformer \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#get numerical columns\n",
    "num_attribs = list( housing_num) \n",
    "#get text columns\n",
    "cat_attribs = [\"ocean_proximity\"] \n",
    "\n",
    "\n",
    "\n",
    "#apply numerical pipelines to num columns portion of data\n",
    "#apply one hot to category column\n",
    "full_pipeline = ColumnTransformer([ (\"num\", num_pipeline, num_attribs), \n",
    "                                   (\"cat\", OneHotEncoder(), cat_attribs),]) \n",
    "\n",
    "#apply the full pipeline to all of the data\n",
    "housing_prepared = full_pipeline.fit_transform( housing)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class RemoveThreeColumns(BaseEstimator, TransformerMixin): \n",
    "    \n",
    "    # no *args or ** kargs\n",
    "    #custom transformer to add the custom attribute \n",
    "    #bedrooms_per_room which combines total_bedrooms/total_rooms columns\n",
    "    def __init__(self, remove_columns = True): \n",
    "        #set boolean true?\n",
    "        self.remove_columns = remove_columns\n",
    "    def fit(self, X, y = None): \n",
    "            # nothing else to do\n",
    "            return self  \n",
    "    def transform(self, X): \n",
    "            \n",
    "            X_ = X.copy()\n",
    "            \n",
    "            X_append = X_.copy()\n",
    "            \n",
    "            X_append = X_append[:, 1]\n",
    "            \n",
    "            X_ = X_[:,0]\n",
    "            \n",
    "            print(\"new X_\")\n",
    "            print(X_)\n",
    "            \n",
    "            return np.c_[X_, X_append]\n",
    "        \n",
    "            #attr_adder = CombinedAttributesAdder( add_bedrooms_per_room = False) \n",
    "            #housing_extra_attribs = attr_adder.transform( housing.values)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline \n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# selector = SelectFromModel(estimator=LogisticRegression()).fit(housing.iloc[:100], housing_labels.iloc[:100])\n",
    "\n",
    "\n",
    "\n",
    "#set the sequence of transformations\n",
    "num_pipeline = Pipeline([ ('imputer', SimpleImputer( strategy =\"median\")), \n",
    "                         ('attribs_adder', CombinedAttributesAdder()), \n",
    "                         ('attribs_remover', RemoveThreeColumns()),\n",
    "                         ('std_scaler', StandardScaler()),]) \n",
    "#apply the sequence\n",
    "housing_num_tr = num_pipeline.fit_transform(housing_num)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#this cell takes the \"ocean_proximity\" column and transforms it\n",
    "#to be numerical like the rest of our columns,\n",
    "#it does so using pipeline\n",
    "#we use \"ColumnTransformer\" as well\n",
    "from sklearn.compose import ColumnTransformer \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#get numerical columns\n",
    "num_attribs = list( housing_num) \n",
    "#get text columns\n",
    "cat_attribs = [[\"ocean_proximity\"] \n",
    "\n",
    "\n",
    "\n",
    "#apply numerical pipelines to num columns portion of data\n",
    "#apply one hot to category column\n",
    "full_pipeline = ColumnTransformer([ (\"num\", num_pipeline, num_attribs), \n",
    "                                   (\"cat\", OneHotEncoder(), cat_attribs),]) \n",
    "\n",
    "#apply the full pipeline to all of the data\n",
    "housing_prepared = full_pipeline.fit_transform( housing)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline \n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# selector = SelectFromModel(estimator=LogisticRegression()).fit(housing.iloc[:100], housing_labels.iloc[:100])\n",
    "\n",
    "\n",
    "\n",
    "#set the sequence of transformations\n",
    "num_pipeline = Pipeline([ ('imputer', SimpleImputer( strategy =\"median\")), \n",
    "                         ('attribs_adder', CombinedAttributesAdder()), \n",
    "                         ('attribs_remover', RemoveThreeColumns()),\n",
    "                         ('std_scaler', StandardScaler()),]) \n",
    "#apply the sequence\n",
    "housing_num_tr = num_pipeline.fit_transform(housing_num)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#this cell takes the \"ocean_proximity\" column and transforms it\n",
    "#to be numerical like the rest of our columns,\n",
    "#it does so using pipeline\n",
    "#we use \"ColumnTransformer\" as well\n",
    "from sklearn.compose import ColumnTransformer \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#get numerical columns\n",
    "num_attribs = list( housing_num) \n",
    "#get text columns\n",
    "cat_attribs = [[\"ocean_proximity\"] \n",
    "\n",
    "\n",
    "\n",
    "#apply numerical pipelines to num columns portion of data\n",
    "#apply one hot to category column\n",
    "full_pipeline = ColumnTransformer([ (\"num\", num_pipeline, num_attribs), \n",
    "                                   (\"cat\", OneHotEncoder(), cat_attribs),]) \n",
    "\n",
    "#apply the full pipeline to all of the data\n",
    "housing_prepared = full_pipeline.fit_transform( housing)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline \n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# selector = SelectFromModel(estimator=LogisticRegression()).fit(housing.iloc[:100], housing_labels.iloc[:100])\n",
    "\n",
    "\n",
    "\n",
    "#set the sequence of transformations\n",
    "num_pipeline = Pipeline([ ('imputer', SimpleImputer( strategy =\"median\")), \n",
    "                         ('attribs_adder', CombinedAttributesAdder()), \n",
    "                         ('attribs_remover', RemoveThreeColumns()),\n",
    "                         ('std_scaler', StandardScaler()),]) \n",
    "#apply the sequence\n",
    "housing_num_tr = num_pipeline.fit_transform(housing_num)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#this cell takes the \"ocean_proximity\" column and transforms it\n",
    "#to be numerical like the rest of our columns,\n",
    "#it does so using pipeline\n",
    "#we use \"ColumnTransformer\" as well\n",
    "from sklearn.compose import ColumnTransformer \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#get numerical columns\n",
    "num_attribs = list( housing_num) \n",
    "#get text columns\n",
    "cat_attribs = [[\"ocean_proximity\"]] \n",
    "\n",
    "\n",
    "\n",
    "#apply numerical pipelines to num columns portion of data\n",
    "#apply one hot to category column\n",
    "full_pipeline = ColumnTransformer([ (\"num\", num_pipeline, num_attribs), \n",
    "                                   (\"cat\", OneHotEncoder(), cat_attribs),]) \n",
    "\n",
    "#apply the full pipeline to all of the data\n",
    "housing_prepared = full_pipeline.fit_transform( housing)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline \n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# selector = SelectFromModel(estimator=LogisticRegression()).fit(housing.iloc[:100], housing_labels.iloc[:100])\n",
    "\n",
    "\n",
    "\n",
    "#set the sequence of transformations\n",
    "num_pipeline = Pipeline([ ('imputer', SimpleImputer( strategy =\"median\")), \n",
    "                         ('attribs_adder', CombinedAttributesAdder()), \n",
    "                         ('attribs_remover', RemoveThreeColumns()),\n",
    "                         ('std_scaler', StandardScaler()),]) \n",
    "#apply the sequence\n",
    "housing_num_tr = num_pipeline.fit_transform(housing_num.toarray().reshape(-1,1))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#this cell takes the \"ocean_proximity\" column and transforms it\n",
    "#to be numerical like the rest of our columns,\n",
    "#it does so using pipeline\n",
    "#we use \"ColumnTransformer\" as well\n",
    "from sklearn.compose import ColumnTransformer \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#get numerical columns\n",
    "num_attribs = list( housing_num) \n",
    "#get text columns\n",
    "cat_attribs = [\"ocean_proximity\"] \n",
    "\n",
    "\n",
    "\n",
    "#apply numerical pipelines to num columns portion of data\n",
    "#apply one hot to category column\n",
    "full_pipeline = ColumnTransformer([ (\"num\", num_pipeline, num_attribs), \n",
    "                                   (\"cat\", OneHotEncoder(), cat_attribs),]) \n",
    "\n",
    "#apply the full pipeline to all of the data\n",
    "housing_prepared = full_pipeline.fit_transform( housing)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class RemoveThreeColumns(BaseEstimator, TransformerMixin): \n",
    "    \n",
    "    # no *args or ** kargs\n",
    "    #custom transformer to add the custom attribute \n",
    "    #bedrooms_per_room which combines total_bedrooms/total_rooms columns\n",
    "    def __init__(self, remove_columns = True): \n",
    "        #set boolean true?\n",
    "        self.remove_columns = remove_columns\n",
    "    def fit(self, X, y = None): \n",
    "            # nothing else to do\n",
    "            return self  \n",
    "    def transform(self, X): \n",
    "            \n",
    "            return X[:, 1:]\n",
    "        \n",
    "            #attr_adder = CombinedAttributesAdder( add_bedrooms_per_room = False) \n",
    "            #housing_extra_attribs = attr_adder.transform( housing.values)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline \n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# selector = SelectFromModel(estimator=LogisticRegression()).fit(housing.iloc[:100], housing_labels.iloc[:100])\n",
    "\n",
    "\n",
    "\n",
    "#set the sequence of transformations\n",
    "num_pipeline = Pipeline([ ('imputer', SimpleImputer( strategy =\"median\")), \n",
    "                         ('attribs_adder', CombinedAttributesAdder()), \n",
    "                         ('attribs_remover', RemoveThreeColumns()),\n",
    "                         ('std_scaler', StandardScaler()),]) \n",
    "#apply the sequence\n",
    "housing_num_tr = num_pipeline.fit_transform(housing_num)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#this cell takes the \"ocean_proximity\" column and transforms it\n",
    "#to be numerical like the rest of our columns,\n",
    "#it does so using pipeline\n",
    "#we use \"ColumnTransformer\" as well\n",
    "from sklearn.compose import ColumnTransformer \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#get numerical columns\n",
    "num_attribs = list( housing_num) \n",
    "#get text columns\n",
    "cat_attribs = [\"ocean_proximity\"] \n",
    "\n",
    "\n",
    "\n",
    "#apply numerical pipelines to num columns portion of data\n",
    "#apply one hot to category column\n",
    "full_pipeline = ColumnTransformer([ (\"num\", num_pipeline, num_attribs), \n",
    "                                   (\"cat\", OneHotEncoder(), cat_attribs),]) \n",
    "\n",
    "#apply the full pipeline to all of the data\n",
    "housing_prepared = full_pipeline.fit_transform( housing)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class RemoveThreeColumns(BaseEstimator, TransformerMixin): \n",
    "    \n",
    "    # no *args or ** kargs\n",
    "    #custom transformer to add the custom attribute \n",
    "    #bedrooms_per_room which combines total_bedrooms/total_rooms columns\n",
    "    def __init__(self, remove_columns = True): \n",
    "        #set boolean true?\n",
    "        self.remove_columns = remove_columns\n",
    "    def fit(self, X, y = None): \n",
    "            # nothing else to do\n",
    "            return self  \n",
    "    def transform(self, X): \n",
    "            \n",
    "            return X[:, 1:]\n",
    "        \n",
    "            #attr_adder = CombinedAttributesAdder( add_bedrooms_per_room = False) \n",
    "            #housing_extra_attribs = attr_adder.transform( housing.values)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline \n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# selector = SelectFromModel(estimator=LogisticRegression()).fit(housing.iloc[:100], housing_labels.iloc[:100])\n",
    "\n",
    "\n",
    "\n",
    "#set the sequence of transformations\n",
    "num_pipeline = Pipeline([ ('imputer', SimpleImputer( strategy =\"median\")), \n",
    "                         ('attribs_adder', CombinedAttributesAdder()), \n",
    "                         ('attribs_remover', RemoveThreeColumns()),\n",
    "                         ('std_scaler', StandardScaler()),]) \n",
    "#apply the sequence\n",
    "housing_num_tr = num_pipeline.fit_transform(housing_num)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#this cell takes the \"ocean_proximity\" column and transforms it\n",
    "#to be numerical like the rest of our columns,\n",
    "#it does so using pipeline\n",
    "#we use \"ColumnTransformer\" as well\n",
    "from sklearn.compose import ColumnTransformer \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#get numerical columns\n",
    "num_attribs = list( housing_num) \n",
    "#get text columns\n",
    "cat_attribs = [\"ocean_proximity\"] \n",
    "\n",
    "\n",
    "\n",
    "#apply numerical pipelines to num columns portion of data\n",
    "#apply one hot to category column\n",
    "full_pipeline = ColumnTransformer([ (\"num\", num_pipeline, num_attribs), \n",
    "                                   (\"cat\", OneHotEncoder(), cat_attribs),]) \n",
    "\n",
    "#apply the full pipeline to all of the data\n",
    "housing_prepared = full_pipeline.fit_transform( housing)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor \n",
    "\n",
    "forest_reg = RandomForestRegressor() \n",
    "forest_reg.fit( housing_prepared, housing_labels)\n",
    "\n",
    "housing_predictions = forest_reg.predict(housing_prepared) \n",
    "forest_mse = mean_squared_error(housing_labels, housing_predictions) \n",
    "forest_rmse = np.sqrt(forest_mse) \n",
    "\n",
    "forest_rmse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_attribs = [\"rooms_per_hhold\", \"pop_per_hhold\", \"bedrooms_per_room\"]\n",
    "cat_encoder = full_pipeline.named_transformers_[\"cat\"] \n",
    "cat_one_hot_attribs = list(cat_encoder.categories_[ 0])\n",
    "attributes = num_attribs + extra_attribs + cat_one_hot_attribs \n",
    "sorted( zip( feature_importances, attributes), reverse = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class RemoveThreeColumns(BaseEstimator, TransformerMixin): \n",
    "    \n",
    "    # no *args or ** kargs\n",
    "    #custom transformer to add the custom attribute \n",
    "    #bedrooms_per_room which combines total_bedrooms/total_rooms columns\n",
    "    def __init__(self, remove_columns = True): \n",
    "        #set boolean true?\n",
    "        self.remove_columns = remove_columns\n",
    "    def fit(self, X, y = None): \n",
    "            # nothing else to do\n",
    "            return self  \n",
    "    def transform(self, X): \n",
    "            \n",
    "            return X[:,1].reshape(-1,1)\n",
    "        \n",
    "            #attr_adder = CombinedAttributesAdder( add_bedrooms_per_room = False) \n",
    "            #housing_extra_attribs = attr_adder.transform( housing.values)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline \n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# selector = SelectFromModel(estimator=LogisticRegression()).fit(housing.iloc[:100], housing_labels.iloc[:100])\n",
    "\n",
    "\n",
    "\n",
    "#set the sequence of transformations\n",
    "num_pipeline = Pipeline([ ('imputer', SimpleImputer( strategy =\"median\")), \n",
    "                         ('attribs_adder', CombinedAttributesAdder()), \n",
    "                         ('attribs_remover', RemoveThreeColumns()),\n",
    "                         ('std_scaler', StandardScaler()),]) \n",
    "#apply the sequence\n",
    "housing_num_tr = num_pipeline.fit_transform(housing_num)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#this cell takes the \"ocean_proximity\" column and transforms it\n",
    "#to be numerical like the rest of our columns,\n",
    "#it does so using pipeline\n",
    "#we use \"ColumnTransformer\" as well\n",
    "from sklearn.compose import ColumnTransformer \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#get numerical columns\n",
    "num_attribs = list( housing_num) \n",
    "#get text columns\n",
    "cat_attribs = [\"ocean_proximity\"] \n",
    "\n",
    "\n",
    "\n",
    "#apply numerical pipelines to num columns portion of data\n",
    "#apply one hot to category column\n",
    "full_pipeline = ColumnTransformer([ (\"num\", num_pipeline, num_attribs), \n",
    "                                   (\"cat\", OneHotEncoder(), cat_attribs),]) \n",
    "\n",
    "#apply the full pipeline to all of the data\n",
    "housing_prepared = full_pipeline.fit_transform( housing)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class RemoveThreeColumns(BaseEstimator, TransformerMixin): \n",
    "    \n",
    "    # no *args or ** kargs\n",
    "    #custom transformer to add the custom attribute \n",
    "    #bedrooms_per_room which combines total_bedrooms/total_rooms columns\n",
    "    def __init__(self, remove_columns = True): \n",
    "        #set boolean true?\n",
    "        self.remove_columns = remove_columns\n",
    "    def fit(self, X, y = None): \n",
    "            # nothing else to do\n",
    "            return self  \n",
    "    def transform(self, X): \n",
    "            \n",
    "            return X[:,1].reshape(-1,1)\n",
    "        \n",
    "            #attr_adder = CombinedAttributesAdder( add_bedrooms_per_room = False) \n",
    "            #housing_extra_attribs = attr_adder.transform( housing.values)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline \n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# selector = SelectFromModel(estimator=LogisticRegression()).fit(housing.iloc[:100], housing_labels.iloc[:100])\n",
    "\n",
    "def all_but_first_column(X):\n",
    "    return X[:, 1:]\n",
    "\n",
    "#set the sequence of transformations\n",
    "num_pipeline = Pipeline([ ('imputer', SimpleImputer( strategy =\"median\")), \n",
    "                         ('attribs_adder', FunctionTransformer(all_but_first_column)), \n",
    "                         ('attribs_remover', RemoveThreeColumns()),\n",
    "                         ('std_scaler', StandardScaler()),]) \n",
    "#apply the sequence\n",
    "housing_num_tr = num_pipeline.fit_transform(housing_num)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#this cell takes the \"ocean_proximity\" column and transforms it\n",
    "#to be numerical like the rest of our columns,\n",
    "#it does so using pipeline\n",
    "#we use \"ColumnTransformer\" as well\n",
    "from sklearn.compose import ColumnTransformer \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#get numerical columns\n",
    "num_attribs = list( housing_num) \n",
    "#get text columns\n",
    "cat_attribs = [\"ocean_proximity\"] \n",
    "\n",
    "\n",
    "\n",
    "#apply numerical pipelines to num columns portion of data\n",
    "#apply one hot to category column\n",
    "full_pipeline = ColumnTransformer([ (\"num\", num_pipeline, num_attribs), \n",
    "                                   (\"cat\", OneHotEncoder(), cat_attribs),]) \n",
    "\n",
    "#apply the full pipeline to all of the data\n",
    "housing_prepared = full_pipeline.fit_transform( housing)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor \n",
    "\n",
    "forest_reg = RandomForestRegressor() \n",
    "forest_reg.fit( housing_prepared, housing_labels)\n",
    "\n",
    "housing_predictions = forest_reg.predict(housing_prepared) \n",
    "forest_mse = mean_squared_error(housing_labels, housing_predictions) \n",
    "forest_rmse = np.sqrt(forest_mse) \n",
    "\n",
    "forest_rmse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_attribs = [\"rooms_per_hhold\", \"pop_per_hhold\", \"bedrooms_per_room\"]\n",
    "cat_encoder = full_pipeline.named_transformers_[\"cat\"] \n",
    "cat_one_hot_attribs = list(cat_encoder.categories_[ 0])\n",
    "attributes = num_attribs + extra_attribs + cat_one_hot_attribs \n",
    "sorted( zip( feature_importances, attributes), reverse = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline \n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# selector = SelectFromModel(estimator=LogisticRegression()).fit(housing.iloc[:100], housing_labels.iloc[:100])\n",
    "\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "def all_but_first_column(X):\n",
    "    return X[:, 1:]\n",
    "\n",
    "#set the sequence of transformations\n",
    "num_pipeline = Pipeline([ ('imputer', SimpleImputer( strategy =\"median\")), \n",
    "                         ('attribs_adder', FunctionTransformer(all_but_first_column)), \n",
    "                         ('attribs_remover', RemoveThreeColumns()),\n",
    "                         ('std_scaler', StandardScaler()),]) \n",
    "#apply the sequence\n",
    "housing_num_tr = num_pipeline.fit_transform(housing_num)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#this cell takes the \"ocean_proximity\" column and transforms it\n",
    "#to be numerical like the rest of our columns,\n",
    "#it does so using pipeline\n",
    "#we use \"ColumnTransformer\" as well\n",
    "from sklearn.compose import ColumnTransformer \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#get numerical columns\n",
    "num_attribs = list( housing_num) \n",
    "#get text columns\n",
    "cat_attribs = [\"ocean_proximity\"] \n",
    "\n",
    "\n",
    "\n",
    "#apply numerical pipelines to num columns portion of data\n",
    "#apply one hot to category column\n",
    "full_pipeline = ColumnTransformer([ (\"num\", num_pipeline, num_attribs), \n",
    "                                   (\"cat\", OneHotEncoder(), cat_attribs),]) \n",
    "\n",
    "#apply the full pipeline to all of the data\n",
    "housing_prepared = full_pipeline.fit_transform( housing)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor \n",
    "\n",
    "forest_reg = RandomForestRegressor() \n",
    "forest_reg.fit( housing_prepared, housing_labels)\n",
    "\n",
    "housing_predictions = forest_reg.predict(housing_prepared) \n",
    "forest_mse = mean_squared_error(housing_labels, housing_predictions) \n",
    "forest_rmse = np.sqrt(forest_mse) \n",
    "\n",
    "forest_rmse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_attribs = [\"rooms_per_hhold\", \"pop_per_hhold\", \"bedrooms_per_room\"]\n",
    "cat_encoder = full_pipeline.named_transformers_[\"cat\"] \n",
    "cat_one_hot_attribs = list(cat_encoder.categories_[ 0])\n",
    "attributes = num_attribs + extra_attribs + cat_one_hot_attribs \n",
    "sorted( zip( feature_importances, attributes), reverse = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#As written in the text, creating the hyper parameter add bedrooms, we are kind of testing\n",
    "#the hyper parameter.. Is it good? well we can first flag it true and test,\n",
    "#then flag it false and test\n",
    "# In this example the transformer has one hyperparameter, add_bedrooms_per_room, \n",
    "# set to True by default (it is often helpful to provide sensible defaults). \n",
    "# This hyperparameter will allow you to easily find out whether adding this\n",
    "# attribute helps the Machine Learning algorithms or not. More generally,\n",
    "# you can add a hyperparameter to gate any data preparation step that you\n",
    "# are not 100% sure about. The more you automate these data preparation steps, \n",
    "# the more combinations you can automatically try out, making it much more likely\n",
    "# that you will find a great combination (and saving you a lot of time).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#for custom transformers three things needed\n",
    "#fit(), transform(), and fit_transform()\n",
    "#note you see 2 below, the third fit_transform()\n",
    "#is included by adding \"TransformerMixin\" as a base class\n",
    "#the \"BaseEstimator\" base class gives two free methods\n",
    "#get_params() and set_params() which are good for auto tuning\n",
    "#hyper params such a hyper param might be the stategy type\n",
    "#of median for the \"SimpleImputer\"\n",
    "from sklearn.base import BaseEstimator, TransformerMixin \n",
    "\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "#these are column indices for the dataset\n",
    "rooms_ix, bedrooms_ix, population_ix, households_ix = 3, 4, 5, 6 \n",
    "\n",
    "class CombinedAttributesAdder(BaseEstimator, TransformerMixin): \n",
    "    \n",
    "    # no *args or ** kargs\n",
    "    #custom transformer to add the custom attribute \n",
    "    #bedrooms_per_room which combines total_bedrooms/total_rooms columns\n",
    "    def __init__(self, add_bedrooms_per_room = True): \n",
    "        #set boolean true?\n",
    "        self.add_bedrooms_per_room = add_bedrooms_per_room \n",
    "    def fit( self, X, y = None): \n",
    "            # nothing else to do\n",
    "            return self  \n",
    "    def transform(self, X): \n",
    "            rooms_per_household = X[:, rooms_ix] / X[:, households_ix] \n",
    "            population_per_household = X[:, population_ix] / X[:, households_ix] \n",
    "            #note here is what np.c_ does\n",
    "            #np.c_[np.array([1,2,3]), np.array([4,5,6])]\n",
    "            #array([[1, 4],\n",
    "            #[2, 5],\n",
    "            #[3, 6]])\n",
    "            #combines columns in input array into 2d array\n",
    "            if self.add_bedrooms_per_room: \n",
    "                bedrooms_per_room = X[:, bedrooms_ix] / X[:, rooms_ix] \n",
    "                \n",
    "                returnVal = np.c_[X, rooms_per_household, population_per_household, bedrooms_per_room] \n",
    "                \n",
    "                sel = VarianceThreshold(threshold=(.8 * (1 - .8)))\n",
    "                sel.fit_transform(returnVal)\n",
    "                \n",
    "                return returnVal\n",
    "            else: \n",
    "                return np.c_[X, rooms_per_household, population_per_household] \n",
    "            \n",
    "            \n",
    "            attr_adder = CombinedAttributesAdder( add_bedrooms_per_room = False) \n",
    "            housing_extra_attribs = attr_adder.transform( housing.values)\n",
    "\n",
    "    \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline \n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# selector = SelectFromModel(estimator=LogisticRegression()).fit(housing.iloc[:100], housing_labels.iloc[:100])\n",
    "\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "def all_but_first_column(X):\n",
    "    return X[:, 1:]\n",
    "\n",
    "#set the sequence of transformations\n",
    "num_pipeline = Pipeline([ ('imputer', SimpleImputer( strategy =\"median\")), \n",
    "                         ('attribs_adder', CombinedAttributesAdder()), \n",
    "                         ('std_scaler', StandardScaler()),]) \n",
    "#apply the sequence\n",
    "housing_num_tr = num_pipeline.fit_transform(housing_num)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#this cell takes the \"ocean_proximity\" column and transforms it\n",
    "#to be numerical like the rest of our columns,\n",
    "#it does so using pipeline\n",
    "#we use \"ColumnTransformer\" as well\n",
    "from sklearn.compose import ColumnTransformer \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#get numerical columns\n",
    "num_attribs = list( housing_num) \n",
    "#get text columns\n",
    "cat_attribs = [\"ocean_proximity\"] \n",
    "\n",
    "\n",
    "\n",
    "#apply numerical pipelines to num columns portion of data\n",
    "#apply one hot to category column\n",
    "full_pipeline = ColumnTransformer([ (\"num\", num_pipeline, num_attribs), \n",
    "                                   (\"cat\", OneHotEncoder(), cat_attribs),]) \n",
    "\n",
    "#apply the full pipeline to all of the data\n",
    "housing_prepared = full_pipeline.fit_transform( housing)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_attribs = [\"rooms_per_hhold\", \"pop_per_hhold\", \"bedrooms_per_room\"]\n",
    "cat_encoder = full_pipeline.named_transformers_[\"cat\"] \n",
    "cat_one_hot_attribs = list(cat_encoder.categories_[ 0])\n",
    "attributes = num_attribs + extra_attribs + cat_one_hot_attribs \n",
    "sorted( zip( feature_importances, attributes), reverse = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#As written in the text, creating the hyper parameter add bedrooms, we are kind of testing\n",
    "#the hyper parameter.. Is it good? well we can first flag it true and test,\n",
    "#then flag it false and test\n",
    "# In this example the transformer has one hyperparameter, add_bedrooms_per_room, \n",
    "# set to True by default (it is often helpful to provide sensible defaults). \n",
    "# This hyperparameter will allow you to easily find out whether adding this\n",
    "# attribute helps the Machine Learning algorithms or not. More generally,\n",
    "# you can add a hyperparameter to gate any data preparation step that you\n",
    "# are not 100% sure about. The more you automate these data preparation steps, \n",
    "# the more combinations you can automatically try out, making it much more likely\n",
    "# that you will find a great combination (and saving you a lot of time).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#for custom transformers three things needed\n",
    "#fit(), transform(), and fit_transform()\n",
    "#note you see 2 below, the third fit_transform()\n",
    "#is included by adding \"TransformerMixin\" as a base class\n",
    "#the \"BaseEstimator\" base class gives two free methods\n",
    "#get_params() and set_params() which are good for auto tuning\n",
    "#hyper params such a hyper param might be the stategy type\n",
    "#of median for the \"SimpleImputer\"\n",
    "from sklearn.base import BaseEstimator, TransformerMixin \n",
    "\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "#these are column indices for the dataset\n",
    "rooms_ix, bedrooms_ix, population_ix, households_ix = 3, 4, 5, 6 \n",
    "\n",
    "class CombinedAttributesAdder(BaseEstimator, TransformerMixin): \n",
    "    \n",
    "    # no *args or ** kargs\n",
    "    #custom transformer to add the custom attribute \n",
    "    #bedrooms_per_room which combines total_bedrooms/total_rooms columns\n",
    "    def __init__(self, add_bedrooms_per_room = True): \n",
    "        #set boolean true?\n",
    "        self.add_bedrooms_per_room = add_bedrooms_per_room \n",
    "    def fit( self, X, y = None): \n",
    "            # nothing else to do\n",
    "            return self  \n",
    "    def transform(self, X): \n",
    "            rooms_per_household = X[:, rooms_ix] / X[:, households_ix] \n",
    "            population_per_household = X[:, population_ix] / X[:, households_ix] \n",
    "            #note here is what np.c_ does\n",
    "            #np.c_[np.array([1,2,3]), np.array([4,5,6])]\n",
    "            #array([[1, 4],\n",
    "            #[2, 5],\n",
    "            #[3, 6]])\n",
    "            #combines columns in input array into 2d array\n",
    "            if self.add_bedrooms_per_room: \n",
    "                bedrooms_per_room = X[:, bedrooms_ix] / X[:, rooms_ix] \n",
    "                \n",
    "                return np.c_[X, rooms_per_household, population_per_household, bedrooms_per_room] \n",
    "            else: \n",
    "                return np.c_[X, rooms_per_household, population_per_household] \n",
    "            \n",
    "            \n",
    "            attr_adder = CombinedAttributesAdder( add_bedrooms_per_room = False) \n",
    "            housing_extra_attribs = attr_adder.transform( housing.values)\n",
    "\n",
    "    \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SimpleImputer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-6c313a59c26f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;31m#set the sequence of transformations\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m num_pipeline = Pipeline([ ('imputer', SimpleImputer(strategy =\"median\")),\n\u001b[0m\u001b[0;32m     16\u001b[0m     \u001b[1;33m(\u001b[0m\u001b[1;34m'feature_selection'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSelectFromModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLinearRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhousing_num\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhousing_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m                             ('classification', RandomForestClassifier())]) \n",
      "\u001b[1;31mNameError\u001b[0m: name 'SimpleImputer' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline \n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# selector = SelectFromModel(estimator=LogisticRegression()).fit(housing.iloc[:100], housing_labels.iloc[:100])\n",
    "\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "def all_but_first_column(X):\n",
    "    return X[:, 1:]\n",
    "\n",
    "#set the sequence of transformations\n",
    "num_pipeline = Pipeline([ ('imputer', SimpleImputer(strategy =\"median\")),\n",
    "    ('feature_selection', SelectFromModel(LinearRegression().fit(housing_num, housing_labels))),\n",
    "                            ('classification', RandomForestClassifier())]) \n",
    "\n",
    "\n",
    "#apply the sequence\n",
    "housing_num_tr = num_pipeline.fit_transform(housing_num)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#this cell takes the \"ocean_proximity\" column and transforms it\n",
    "#to be numerical like the rest of our columns,\n",
    "#it does so using pipeline\n",
    "#we use \"ColumnTransformer\" as well\n",
    "from sklearn.compose import ColumnTransformer \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#get numerical columns\n",
    "num_attribs = list( housing_num) \n",
    "#get text columns\n",
    "cat_attribs = [\"ocean_proximity\"] \n",
    "\n",
    "\n",
    "\n",
    "#apply numerical pipelines to num columns portion of data\n",
    "#apply one hot to category column\n",
    "full_pipeline = ColumnTransformer([ (\"num\", num_pipeline, num_attribs), \n",
    "                                   (\"cat\", OneHotEncoder(), cat_attribs),]) \n",
    "\n",
    "#apply the full pipeline to all of the data\n",
    "housing_prepared = full_pipeline.fit_transform( housing)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(housing_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline \n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# selector = SelectFromModel(estimator=LogisticRegression()).fit(housing.iloc[:100], housing_labels.iloc[:100])\n",
    "\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "def all_but_first_column(X):\n",
    "    return X[:, 1:]\n",
    "\n",
    "#set the sequence of transformations\n",
    "\n",
    "\n",
    "num_pipeline = Pipeline([ ('imputer', SimpleImputer(strategy =\"median\")),\n",
    "                        ('feature_selection', SelectFromModel(LinearSVC())), \n",
    "                          \n",
    "                        ]) \n",
    "\n",
    "\n",
    "#apply the sequence\n",
    "housing_num_tr = num_pipeline.fit_transform(housing_num)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#this cell takes the \"ocean_proximity\" column and transforms it\n",
    "#to be numerical like the rest of our columns,\n",
    "#it does so using pipeline\n",
    "#we use \"ColumnTransformer\" as well\n",
    "from sklearn.compose import ColumnTransformer \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#get numerical columns\n",
    "num_attribs = list( housing_num) \n",
    "#get text columns\n",
    "cat_attribs = [\"ocean_proximity\"] \n",
    "\n",
    "\n",
    "\n",
    "#apply numerical pipelines to num columns portion of data\n",
    "#apply one hot to category column\n",
    "full_pipeline = ColumnTransformer([ (\"num\", num_pipeline, num_attribs), \n",
    "                                   (\"cat\", OneHotEncoder(), cat_attribs),]) \n",
    "\n",
    "#apply the full pipeline to all of the data\n",
    "housing_prepared = full_pipeline.fit_transform( housing)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline \n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# selector = SelectFromModel(estimator=LogisticRegression()).fit(housing.iloc[:100], housing_labels.iloc[:100])\n",
    "\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "def all_but_first_column(X):\n",
    "    return X[:, 1:]\n",
    "\n",
    "#set the sequence of transformations\n",
    "num_pipeline = Pipeline([ ('imputer', SimpleImputer(strategy =\"median\")),\n",
    "    ('feature_selection', SelectFromModel(LinearSVC())),\n",
    "                            ('classification', RandomForestClassifier())]) \n",
    "\n",
    "\n",
    "#apply the sequence\n",
    "housing_num_tr = num_pipeline.fit_transform(housing_num)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#this cell takes the \"ocean_proximity\" column and transforms it\n",
    "#to be numerical like the rest of our columns,\n",
    "#it does so using pipeline\n",
    "#we use \"ColumnTransformer\" as well\n",
    "from sklearn.compose import ColumnTransformer \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#get numerical columns\n",
    "num_attribs = list( housing_num) \n",
    "#get text columns\n",
    "cat_attribs = [\"ocean_proximity\"] \n",
    "\n",
    "\n",
    "\n",
    "#apply numerical pipelines to num columns portion of data\n",
    "#apply one hot to category column\n",
    "full_pipeline = ColumnTransformer([ (\"num\", num_pipeline, num_attribs), \n",
    "                                   (\"cat\", OneHotEncoder(), cat_attribs),]) \n",
    "\n",
    "#apply the full pipeline to all of the data\n",
    "housing_prepared = full_pipeline.fit_transform( housing)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_num.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_tr.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline \n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# selector = SelectFromModel(estimator=LogisticRegression()).fit(housing.iloc[:100], housing_labels.iloc[:100])\n",
    "\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "def all_but_first_column(X):\n",
    "    return X[:, 1:]\n",
    "\n",
    "#set the sequence of transformations\n",
    "num_pipeline = Pipeline([ ('imputer', SimpleImputer(strategy =\"median\")),\n",
    "    ('feature_selection', SelectFromModel(LinearRegression().fit(housing_tr, housing_labels))),\n",
    "                            ('classification', RandomForestClassifier())]) \n",
    "\n",
    "\n",
    "#apply the sequence\n",
    "housing_num_tr = num_pipeline.fit_transform(housing_num)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#this cell takes the \"ocean_proximity\" column and transforms it\n",
    "#to be numerical like the rest of our columns,\n",
    "#it does so using pipeline\n",
    "#we use \"ColumnTransformer\" as well\n",
    "from sklearn.compose import ColumnTransformer \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#get numerical columns\n",
    "num_attribs = list( housing_num) \n",
    "#get text columns\n",
    "cat_attribs = [\"ocean_proximity\"] \n",
    "\n",
    "\n",
    "\n",
    "#apply numerical pipelines to num columns portion of data\n",
    "#apply one hot to category column\n",
    "full_pipeline = ColumnTransformer([ (\"num\", num_pipeline, num_attribs), \n",
    "                                   (\"cat\", OneHotEncoder(), cat_attribs),]) \n",
    "\n",
    "#apply the full pipeline to all of the data\n",
    "housing_prepared = full_pipeline.fit_transform( housing)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline \n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# selector = SelectFromModel(estimator=LogisticRegression()).fit(housing.iloc[:100], housing_labels.iloc[:100])\n",
    "\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "def all_but_last_column(X):\n",
    "    return X[:,:-1]\n",
    "\n",
    "#set the sequence of transformations\n",
    "num_pipeline = Pipeline([ ('imputer', SimpleImputer(strategy =\"median\")),\n",
    "                         ('remove_lastcol', FunctionTransformer(all_but_last_column)),\n",
    "    ('feature_selection', SelectFromModel(LinearSVC(fit_intercept=\"false\"))),\n",
    "                            ('classification', RandomForestClassifier())]) \n",
    "\n",
    "\n",
    "#apply the sequence\n",
    "housing_num_tr = num_pipeline.fit_transform(housing_num)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#this cell takes the \"ocean_proximity\" column and transforms it\n",
    "#to be numerical like the rest of our columns,\n",
    "#it does so using pipeline\n",
    "#we use \"ColumnTransformer\" as well\n",
    "from sklearn.compose import ColumnTransformer \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#get numerical columns\n",
    "num_attribs = list( housing_num) \n",
    "#get text columns\n",
    "cat_attribs = [\"ocean_proximity\"] \n",
    "\n",
    "\n",
    "\n",
    "#apply numerical pipelines to num columns portion of data\n",
    "#apply one hot to category column\n",
    "full_pipeline = ColumnTransformer([ (\"num\", num_pipeline, num_attribs), \n",
    "                                   (\"cat\", OneHotEncoder(), cat_attribs),]) \n",
    "\n",
    "#apply the full pipeline to all of the data\n",
    "housing_prepared = full_pipeline.fit_transform( housing)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dumbcopy = housing_tr.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dumbcopy.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_but_last_column(dumbcopy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dumbcopy.drop(\"latitude\", axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dumbcopy.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_num.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline \n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# selector = SelectFromModel(estimator=LogisticRegression()).fit(housing.iloc[:100], housing_labels.iloc[:100])\n",
    "\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "def all_but_last_column(X):\n",
    "    return X[:,:-1]\n",
    "\n",
    "#set the sequence of transformations\n",
    "num_pipeline = Pipeline([ ('imputer', SimpleImputer(strategy =\"median\")),\n",
    "                         \n",
    "    ('feature_selection', SelectFromModel(LinearSVC(fit_intercept=\"false\"))),\n",
    "                            ('classification', RandomForestClassifier())]) \n",
    "\n",
    "\n",
    "#apply the sequence\n",
    "housing_num_tr = num_pipeline.fit_transform(housing_num)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#this cell takes the \"ocean_proximity\" column and transforms it\n",
    "#to be numerical like the rest of our columns,\n",
    "#it does so using pipeline\n",
    "#we use \"ColumnTransformer\" as well\n",
    "from sklearn.compose import ColumnTransformer \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#get numerical columns\n",
    "num_attribs = list( housing_num) \n",
    "#get text columns\n",
    "cat_attribs = [\"ocean_proximity\"] \n",
    "\n",
    "\n",
    "\n",
    "#apply numerical pipelines to num columns portion of data\n",
    "#apply one hot to category column\n",
    "full_pipeline = ColumnTransformer([ (\"num\", num_pipeline, num_attribs), \n",
    "                                   (\"cat\", OneHotEncoder(), cat_attribs),]) \n",
    "\n",
    "#apply the full pipeline to all of the data\n",
    "housing_prepared = full_pipeline.fit_transform( housing)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_num_tr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_num.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median = housing[\"total_bedrooms\"]. median()\n",
    "\n",
    "from sklearn.impute import SimpleImputer \n",
    "imputer = SimpleImputer( strategy =\"median\")\n",
    "\n",
    "\n",
    "housing_num = housing.drop(\"ocean_proximity\", axis = 1)\n",
    "\n",
    "imputer.fit(housing_num)\n",
    "\n",
    "X = imputer.transform(housing_num)\n",
    "\n",
    "housing_num = pd.DataFrame(X, columns = housing_num.columns, index = housing_num.index)\n",
    "\n",
    "housing_num.info()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline \n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# selector = SelectFromModel(estimator=LogisticRegression()).fit(housing.iloc[:100], housing_labels.iloc[:100])\n",
    "\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "def all_but_last_column(X):\n",
    "    return X[:,:-1]\n",
    "\n",
    "#set the sequence of transformations\n",
    "num_pipeline = Pipeline([ \n",
    "                         \n",
    "    ('feature_selection', SelectFromModel(LogisticRegression()).fit(housing_num, housing_labels)),\n",
    "                            ('classification', RandomForestClassifier())]) \n",
    "\n",
    "\n",
    "#apply the sequence\n",
    "housing_num_tr = num_pipeline.fit_transform(housing_num)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#this cell takes the \"ocean_proximity\" column and transforms it\n",
    "#to be numerical like the rest of our columns,\n",
    "#it does so using pipeline\n",
    "#we use \"ColumnTransformer\" as well\n",
    "from sklearn.compose import ColumnTransformer \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#get numerical columns\n",
    "num_attribs = list( housing_num) \n",
    "#get text columns\n",
    "cat_attribs = [\"ocean_proximity\"] \n",
    "\n",
    "\n",
    "\n",
    "#apply numerical pipelines to num columns portion of data\n",
    "#apply one hot to category column\n",
    "full_pipeline = ColumnTransformer([ (\"num\", num_pipeline, num_attribs), \n",
    "                                   (\"cat\", OneHotEncoder(), cat_attribs),]) \n",
    "\n",
    "#apply the full pipeline to all of the data\n",
    "housing_prepared = full_pipeline.fit_transform( housing)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor \n",
    "\n",
    "forest_reg = RandomForestRegressor() \n",
    "forest_reg.fit( housing_prepared, housing_labels)\n",
    "\n",
    "housing_predictions = forest_reg.predict(housing_prepared) \n",
    "forest_mse = mean_squared_error(housing_labels, housing_predictions) \n",
    "forest_rmse = np.sqrt(forest_mse) \n",
    "\n",
    "forest_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_attribs = list( housing_num_tr)\n",
    "attributes = num_attribs \n",
    "sorted( zip( feature_importances, attributes), reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor \n",
    "\n",
    "forest_reg = RandomForestRegressor() \n",
    "forest_reg.fit( housing_prepared, housing_labels)\n",
    "\n",
    "housing_predictions = forest_reg.predict(housing_prepared) \n",
    "forest_mse = mean_squared_error(housing_labels, housing_predictions) \n",
    "forest_rmse = np.sqrt(forest_mse) \n",
    "\n",
    "forest_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
